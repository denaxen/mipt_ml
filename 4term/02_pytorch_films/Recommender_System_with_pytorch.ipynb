{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Recommender_System_with_pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgKRbws6TBBA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee408e4d-e1e5-4505-8ad1-804d2921c939"
      },
      "source": [
        "\n",
        "# all config/downloads to use fastai\n",
        "# !pip install pandas --upgrade\n",
        "#!pip install plotly --upgrade\n",
        "#!pip install fastai==0.7.0\n",
        "#!pip install torchtext==0.2.3\n",
        "!pip install torch\n",
        "#!pip install torchvision\n",
        "#!pip install Pillow>=4.1.1\n",
        "#!pip install image\n",
        "#!pip install matplotlib"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.7.1+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJxqzk8tXTYt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6b414c9-3f33-4470-d954-86e5a60e7b5d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/mnt')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /mnt; to attempt to forcibly remount, call drive.mount(\"/mnt\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwX4CH-aYclb"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnS13iIyYgbS"
      },
      "source": [
        "data_pd = pd.read_csv(\"/mnt/MyDrive/datasets/ratings.csv\")\n",
        "movies_pd = pd.read_csv('/mnt/MyDrive/datasets/movies.csv')\n",
        "data_pd = data_pd.sample(frac=1)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "v2rbCWdbuInX",
        "outputId": "0cf5c32a-e394-4ea7-d58c-6ae890bba379"
      },
      "source": [
        "data_pd.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>userId</th>\n",
              "      <th>movieId</th>\n",
              "      <th>rating</th>\n",
              "      <th>timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6363</th>\n",
              "      <td>43</td>\n",
              "      <td>317</td>\n",
              "      <td>4.0</td>\n",
              "      <td>848993903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9973</th>\n",
              "      <td>65</td>\n",
              "      <td>97752</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1494767354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98862</th>\n",
              "      <td>608</td>\n",
              "      <td>1200</td>\n",
              "      <td>4.5</td>\n",
              "      <td>1117491293</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89632</th>\n",
              "      <td>580</td>\n",
              "      <td>6645</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1167861051</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3589</th>\n",
              "      <td>21</td>\n",
              "      <td>110102</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1407619805</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       userId  movieId  rating   timestamp\n",
              "6363       43      317     4.0   848993903\n",
              "9973       65    97752     4.0  1494767354\n",
              "98862     608     1200     4.5  1117491293\n",
              "89632     580     6645     4.0  1167861051\n",
              "3589       21   110102     3.5  1407619805"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "FWDUhw-guJ9l",
        "outputId": "cbb34727-1fb5-4322-87a3-ea77f5be2e2b"
      },
      "source": [
        "movies_pd.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>movieId</th>\n",
              "      <th>title</th>\n",
              "      <th>genres</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Toy Story (1995)</td>\n",
              "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Jumanji (1995)</td>\n",
              "      <td>Adventure|Children|Fantasy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Grumpier Old Men (1995)</td>\n",
              "      <td>Comedy|Romance</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Waiting to Exhale (1995)</td>\n",
              "      <td>Comedy|Drama|Romance</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Father of the Bride Part II (1995)</td>\n",
              "      <td>Comedy</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   movieId  ...                                       genres\n",
              "0        1  ...  Adventure|Animation|Children|Comedy|Fantasy\n",
              "1        2  ...                   Adventure|Children|Fantasy\n",
              "2        3  ...                               Comedy|Romance\n",
              "3        4  ...                         Comedy|Drama|Romance\n",
              "4        5  ...                                       Comedy\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07dBCA5_hyL5"
      },
      "source": [
        "u_temp = list(data_pd.userId.unique())\n",
        "u_temp.sort()\n",
        "\n",
        "m_temp = list(data_pd.movieId.unique())\n",
        "m_temp.sort()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCX_KoefjiAS"
      },
      "source": [
        "movie_dict = dict(list(zip(m_temp,range(len(m_temp)))))\n",
        "user_dict = dict(list(zip(u_temp,range(len(u_temp)))))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObuwAPkDZ_Fe"
      },
      "source": [
        "data_pd['userId'] = data_pd['userId'].map(lambda x: user_dict[x])\n",
        "data_pd['movieId'] = data_pd['movieId'].map(lambda x: movie_dict[x])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5h407gkZvyA"
      },
      "source": [
        "X = data_pd[['userId','movieId']].values\n",
        "y = data_pd[['rating']].values"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "na_mFKUInRp1"
      },
      "source": [
        "y = y.astype(np.float)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajDDMIY0lSMb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cab6663c-004c-4f62-e360-d7355a4b5d7e"
      },
      "source": [
        "type(np.double(1.))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wU9d0QZZseE"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVqfwxArTH_k"
      },
      "source": [
        "class RatingDataset():\n",
        "  def __init__(self, train, label):\n",
        "    self.feature_= train\n",
        "    self.label_= label\n",
        "  def __len__(self):\n",
        "    #return size of dataset\n",
        "    return len(self.feature_)\n",
        "  def __getitem__(self, idx):\n",
        "    return torch.tensor(self.feature_[idx]),torch.tensor(self.label_[idx])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJ45RoUpaji6"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTYTBi2oTybM"
      },
      "source": [
        "bs = 1000\n",
        "train_dataloader = DataLoader(RatingDataset(x_train, y_train), batch_size=bs, shuffle=True)\n",
        "test_dataloader = DataLoader(RatingDataset(x_test, y_test), batch_size=bs)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ibe_sZaqTye6"
      },
      "source": [
        "class MatrixFactorization(torch.nn.Module):\n",
        "    \n",
        "    def __init__(self, n_users, n_items, n_factors=20):\n",
        "        super().__init__()\n",
        "        self.user_factors = torch.nn.Embedding(n_users, n_factors)\n",
        "        self.item_factors = torch.nn.Embedding(n_items, n_factors)\n",
        "        self.user_biases = torch.nn.Embedding(n_users, 1)\n",
        "        self.item_biases = torch.nn.Embedding(n_items,1)\n",
        "        torch.nn.init.xavier_uniform_(self.user_factors.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.item_factors.weight)\n",
        "        self.user_biases.weight.data.fill_(0.)\n",
        "        self.item_biases.weight.data.fill_(0.)\n",
        "        \n",
        "    def forward(self, user, item):\n",
        "        pred = self.user_biases(user) + self.item_biases(item)\n",
        "        pred += (self.user_factors(user) * self.item_factors(item)).sum(1, keepdim=True)\n",
        "        #pred = pred.float()\n",
        "        return pred.squeeze()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAaMTj_La9sl"
      },
      "source": [
        "n,m =  len(data_pd.userId.unique()), len(data_pd.movieId.unique())"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwfzQF1TTyiI"
      },
      "source": [
        "nfactor = 100\n",
        "model = MatrixFactorization(n, m, n_factors=nfactor)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSp-CtZVbXiS"
      },
      "source": [
        "dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "# dev = torch.device(\"cpu\")"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQdt3LMabZOT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd1fb7fa-6b16-4166-eff9-b7d9e8762491"
      },
      "source": [
        "model.parameters"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Module.parameters of MatrixFactorization(\n",
              "  (user_factors): Embedding(610, 100)\n",
              "  (item_factors): Embedding(9724, 100)\n",
              "  (user_biases): Embedding(610, 1)\n",
              "  (item_biases): Embedding(9724, 1)\n",
              ")>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cLD_k_zhPp4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cc3dfd4-4eb6-44b2-dfff-f489fb85ee1e"
      },
      "source": [
        "dev"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpKn6O-chTs4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b8d344a-f82f-4497-f69f-5bd660ce2ce4"
      },
      "source": [
        "loss_func = torch.nn.MSELoss()\n",
        "model.to(dev)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MatrixFactorization(\n",
              "  (user_factors): Embedding(610, 100)\n",
              "  (item_factors): Embedding(9724, 100)\n",
              "  (user_biases): Embedding(610, 1)\n",
              "  (item_biases): Embedding(9724, 1)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dx1Zyej08C2z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "050d4a0a-97a1-4841-d002-95b02d67bfed"
      },
      "source": [
        "model.parameters\r\n",
        "model.double()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MatrixFactorization(\n",
              "  (user_factors): Embedding(610, 100)\n",
              "  (item_factors): Embedding(9724, 100)\n",
              "  (user_biases): Embedding(610, 1)\n",
              "  (item_biases): Embedding(9724, 1)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zGp8094bQTa",
        "outputId": "fd73228b-70fe-4999-a777-9f923167df77"
      },
      "source": [
        "train_losses = []\n",
        "test_losses = []\n",
        "epoches_list = []\n",
        "\n",
        "epoches = 1000\n",
        "val_loss = 0\n",
        "count_epoch = 0\n",
        "last_loss = 0\n",
        "min_delta = 0.005\n",
        "min_val_loss = 1.5\n",
        "min_loss = 1.2\n",
        "n_epochs_stop = 20\n",
        "for epoch in range(0,epoches):\n",
        "    pbar = tqdm(enumerate(train_dataloader), total=len(train_dataloader))  # progress bar\n",
        "    count = 0\n",
        "    cum_loss = 0.\n",
        "    for i,( train_batch, label_batch) in pbar:\n",
        "        count = 1 + i\n",
        "        # Predict and calculate loss for user factor and bias\n",
        "        optimizer = torch.optim.SGD([model.user_biases.weight,model.user_factors.weight], lr=0.01, weight_decay=1e-5) # learning rate\n",
        "        prediction = model(train_batch[:,0].to(dev), train_batch[:,1].to(dev))\n",
        "        \n",
        "        loss = loss_func(prediction, label_batch.to(dev))    \n",
        "        # Backpropagate\n",
        "        loss.backward()\n",
        "        \n",
        "        # Update the parameters\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        #predict and calculate loss for item factor and bias\n",
        "        optimizer = torch.optim.SGD([model.item_biases.weight,model.item_factors.weight], lr=0.01, weight_decay=1e-5) # learning rate\n",
        "        prediction = model(train_batch[:,0].to(dev), train_batch[:,1].to(dev))\n",
        "        loss = loss_func(prediction, label_batch.to(dev))\n",
        "        # Backpropagate\n",
        "        loss.backward()\n",
        "        \n",
        "        # Update the parameters\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        cum_loss += loss.item()\n",
        "        pbar.set_description('training loss at {} batch {}: {}'.format(epoch,i,loss.item()))\n",
        "    train_loss = cum_loss/count\n",
        "    val_loss += loss\n",
        "    val_loss = val_loss / len(train_dataloader)\n",
        "\n",
        "    delta = train_loss - last_loss\n",
        "    if abs(delta) < abs(min_delta):\n",
        "      count_epoch += 1\n",
        "    if count_epoch == n_epochs_stop:\n",
        "      print('\\n EARLY STOP!')\n",
        "      break\n",
        "    if delta > min_delta:\n",
        "      count_epoch = 0\n",
        "\n",
        "    last_loss = train_loss\n",
        "    pbar = tqdm(enumerate(test_dataloader), total=len(test_dataloader))  # progress bar\n",
        "    cum_loss =0.\n",
        "    count = 0\n",
        "    for i,( test_batch, label_batch) in pbar:\n",
        "        count = 1 + i\n",
        "        with torch.no_grad():\n",
        "            prediction = model(test_batch[:,0].to(dev), test_batch[:,1].to(dev))\n",
        "            loss = loss_func(prediction, label_batch.to(dev))\n",
        "            cum_loss += loss.item()\n",
        "            pbar.set_description('test loss at {} batch {}: {}'.format(epoch,i,loss.item()))\n",
        "    test_loss = cum_loss/count\n",
        "\n",
        "    test_losses.append(test_loss)\n",
        "    train_losses.append(train_loss)\n",
        "    epoches_list.append(epoch)\n",
        "    print('avg training loss: ', train_loss, ' avg test loss: ',test_loss)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/81 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:446: UserWarning: Using a target size (torch.Size([1000, 1])) that is different to the input size (torch.Size([1000])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "training loss at 0 batch 79: 12.747178107011447:  96%|█████████▋| 78/81 [00:02<00:00, 29.59it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:446: UserWarning: Using a target size (torch.Size([668, 1])) that is different to the input size (torch.Size([668])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "training loss at 0 batch 80: 12.655817630032923: 100%|██████████| 81/81 [00:02<00:00, 27.87it/s]\n",
            "test loss at 0 batch 19: 12.877890422621174:  86%|████████▌ | 18/21 [00:00<00:00, 32.39it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:446: UserWarning: Using a target size (torch.Size([168, 1])) that is different to the input size (torch.Size([168])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "test loss at 0 batch 20: 12.726779612658241: 100%|██████████| 21/21 [00:00<00:00, 34.48it/s]\n",
            "training loss at 1 batch 4: 12.897076851807178:   5%|▍         | 4/81 [00:00<00:02, 28.78it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  13.079297643005635  avg test loss:  12.90033095962017\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 1 batch 80: 12.487388212016619: 100%|██████████| 81/81 [00:03<00:00, 26.57it/s]\n",
            "test loss at 1 batch 20: 12.282642085219072: 100%|██████████| 21/21 [00:00<00:00, 24.80it/s]\n",
            "training loss at 2 batch 4: 12.373283688401477:   4%|▎         | 3/81 [00:00<00:02, 29.00it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  12.596933837460726  avg test loss:  12.439928133458213\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 2 batch 80: 11.741579791034672: 100%|██████████| 81/81 [00:03<00:00, 25.32it/s]\n",
            "test loss at 2 batch 20: 11.870287408599133: 100%|██████████| 21/21 [00:00<00:00, 37.77it/s]\n",
            "training loss at 3 batch 4: 11.856462064422795:   4%|▎         | 3/81 [00:00<00:02, 29.90it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  12.148801040964512  avg test loss:  12.013853231663505\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 3 batch 80: 11.338564605805692: 100%|██████████| 81/81 [00:03<00:00, 26.17it/s]\n",
            "test loss at 3 batch 20: 11.485829272002764: 100%|██████████| 21/21 [00:00<00:00, 38.17it/s]\n",
            "training loss at 4 batch 4: 11.636600531316939:   5%|▍         | 4/81 [00:00<00:02, 32.18it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  11.734292886556489  avg test loss:  11.617807406918754\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 4 batch 80: 11.304815894504015: 100%|██████████| 81/81 [00:03<00:00, 25.78it/s]\n",
            "test loss at 4 batch 20: 11.126395922939322: 100%|██████████| 21/21 [00:00<00:00, 35.71it/s]\n",
            "training loss at 5 batch 4: 11.095034708200991:   4%|▎         | 3/81 [00:00<00:02, 29.70it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  11.350442536674976  avg test loss:  11.248851023247052\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 5 batch 80: 10.841378639690987: 100%|██████████| 81/81 [00:02<00:00, 28.26it/s]\n",
            "test loss at 5 batch 20: 10.789847482007273: 100%|██████████| 21/21 [00:00<00:00, 31.22it/s]\n",
            "training loss at 6 batch 5: 10.98040684316396:   5%|▍         | 4/81 [00:00<00:02, 30.87it/s] "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  10.990831058143264  avg test loss:  10.904391224444904\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 6 batch 80: 10.548236098138117: 100%|██████████| 81/81 [00:02<00:00, 27.32it/s]\n",
            "test loss at 6 batch 20: 10.47386420492019: 100%|██████████| 21/21 [00:00<00:00, 31.21it/s]\n",
            "training loss at 7 batch 3: 10.329965870389152:   4%|▎         | 3/81 [00:00<00:03, 22.00it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  10.655556676146318  avg test loss:  10.581958448404315\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 7 batch 80: 10.13979342120145: 100%|██████████| 81/81 [00:03<00:00, 25.02it/s]\n",
            "test loss at 7 batch 20: 10.176413664796865: 100%|██████████| 21/21 [00:00<00:00, 32.81it/s]\n",
            "training loss at 8 batch 4: 9.900027936279942:   5%|▍         | 4/81 [00:00<00:02, 31.71it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  10.340859868347176  avg test loss:  10.279532263709502\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 8 batch 80: 9.492007119883178: 100%|██████████| 81/81 [00:03<00:00, 24.43it/s]\n",
            "test loss at 8 batch 20: 9.896176967565479: 100%|██████████| 21/21 [00:00<00:00, 34.33it/s]\n",
            "training loss at 9 batch 4: 9.62914263358171:   5%|▍         | 4/81 [00:00<00:02, 31.33it/s] "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  10.044778550753474  avg test loss:  9.995200154222928\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 9 batch 80: 9.878639918919264: 100%|██████████| 81/81 [00:03<00:00, 26.38it/s]\n",
            "test loss at 9 batch 20: 9.631416268132625: 100%|██████████| 21/21 [00:00<00:00, 26.47it/s]\n",
            "training loss at 10 batch 2: 9.229407387864345:   2%|▏         | 2/81 [00:00<00:04, 17.00it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  9.770116150318898  avg test loss:  9.72728294019133\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 10 batch 80: 9.179781032748789: 100%|██████████| 81/81 [00:03<00:00, 24.63it/s]\n",
            "test loss at 10 batch 20: 9.38084299155203: 100%|██████████| 21/21 [00:00<00:00, 36.31it/s]\n",
            "training loss at 11 batch 4: 9.676826644111793:   4%|▎         | 3/81 [00:00<00:03, 25.95it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  9.50690892694942  avg test loss:  9.47442112679028\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 11 batch 80: 9.158007138525663: 100%|██████████| 81/81 [00:03<00:00, 26.92it/s]\n",
            "test loss at 11 batch 20: 9.143489471086898: 100%|██████████| 21/21 [00:00<00:00, 31.90it/s]\n",
            "training loss at 12 batch 4: 8.944707610261851:   4%|▎         | 3/81 [00:00<00:02, 29.58it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  9.260776114474707  avg test loss:  9.23533443671242\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 12 batch 80: 8.933436622859315: 100%|██████████| 81/81 [00:03<00:00, 24.15it/s]\n",
            "test loss at 12 batch 20: 8.91814080254559: 100%|██████████| 21/21 [00:00<00:00, 25.66it/s]\n",
            "training loss at 13 batch 4: 8.797533766482966:   4%|▎         | 3/81 [00:00<00:03, 24.23it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  9.027205894817401  avg test loss:  9.008838658524997\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 13 batch 80: 9.204600253042917: 100%|██████████| 81/81 [00:02<00:00, 28.05it/s]\n",
            "test loss at 13 batch 20: 8.703701879417098: 100%|██████████| 21/21 [00:00<00:00, 27.38it/s]\n",
            "training loss at 14 batch 2: 8.56305864847163:   4%|▎         | 3/81 [00:00<00:03, 20.14it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  8.808238679073801  avg test loss:  8.793920714721104\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 14 batch 80: 8.581156179986017: 100%|██████████| 81/81 [00:03<00:00, 25.14it/s]\n",
            "test loss at 14 batch 20: 8.499867676413889: 100%|██████████| 21/21 [00:00<00:00, 28.05it/s]\n",
            "training loss at 15 batch 2: 8.442975874870069:   2%|▏         | 2/81 [00:00<00:04, 17.16it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  8.59644476485069  avg test loss:  8.589708203238317\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 15 batch 80: 8.577697855525308: 100%|██████████| 81/81 [00:03<00:00, 22.11it/s]\n",
            "test loss at 15 batch 20: 8.30537763755311: 100%|██████████| 21/21 [00:00<00:00, 35.54it/s]\n",
            "training loss at 16 batch 4: 8.470035283015564:   4%|▎         | 3/81 [00:00<00:02, 29.77it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  8.397632564525486  avg test loss:  8.395388989800301\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 16 batch 80: 7.8041832778879545: 100%|██████████| 81/81 [00:03<00:00, 25.86it/s]\n",
            "test loss at 16 batch 20: 8.119897812659154: 100%|██████████| 21/21 [00:00<00:00, 31.60it/s]\n",
            "training loss at 17 batch 4: 8.390809640579727:   5%|▍         | 4/81 [00:00<00:02, 31.74it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  8.205105966609365  avg test loss:  8.2102064154669\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 17 batch 80: 7.703588963659337: 100%|██████████| 81/81 [00:03<00:00, 24.97it/s]\n",
            "test loss at 17 batch 20: 7.942539603963354: 100%|██████████| 21/21 [00:00<00:00, 28.77it/s]\n",
            "training loss at 18 batch 4: 8.070239806636202:   4%|▎         | 3/81 [00:00<00:02, 29.02it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  8.024463195853814  avg test loss:  8.033497780412437\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 18 batch 80: 7.956400298851452: 100%|██████████| 81/81 [00:03<00:00, 25.83it/s]\n",
            "test loss at 18 batch 20: 7.772994342747722: 100%|██████████| 21/21 [00:00<00:00, 36.81it/s]\n",
            "training loss at 19 batch 4: 7.762801556826159:   5%|▍         | 4/81 [00:00<00:02, 31.09it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  7.853516771957757  avg test loss:  7.864727218806808\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 19 batch 80: 7.632355670403816: 100%|██████████| 81/81 [00:03<00:00, 26.43it/s]\n",
            "test loss at 19 batch 20: 7.610552941913164: 100%|██████████| 21/21 [00:00<00:00, 34.78it/s]\n",
            "training loss at 20 batch 3: 7.397877180607968:   4%|▎         | 3/81 [00:00<00:02, 26.71it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  7.6875043171304664  avg test loss:  7.703230183020798\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 20 batch 80: 7.556209047675888: 100%|██████████| 81/81 [00:03<00:00, 25.57it/s]\n",
            "test loss at 20 batch 20: 7.454955157832518: 100%|██████████| 21/21 [00:00<00:00, 37.09it/s]\n",
            "training loss at 21 batch 4: 7.4395021249877065:   4%|▎         | 3/81 [00:00<00:02, 29.83it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  7.53000123994622  avg test loss:  7.54871773545115\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 21 batch 80: 6.63637171908046: 100%|██████████| 81/81 [00:03<00:00, 25.42it/s]\n",
            "test loss at 21 batch 20: 7.305681962402406: 100%|██████████| 21/21 [00:00<00:00, 34.51it/s]\n",
            "training loss at 22 batch 4: 7.300696920011557:   5%|▍         | 4/81 [00:00<00:02, 30.48it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  7.375798716726935  avg test loss:  7.400575758586908\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 22 batch 80: 7.180142162950978: 100%|██████████| 81/81 [00:03<00:00, 24.22it/s]\n",
            "test loss at 22 batch 20: 7.162375136903481: 100%|██████████| 21/21 [00:00<00:00, 34.61it/s]\n",
            "training loss at 23 batch 4: 7.073113266458695:   4%|▎         | 3/81 [00:00<00:02, 28.74it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  7.233407475848497  avg test loss:  7.25847156378545\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 23 batch 80: 7.3143571788580495: 100%|██████████| 81/81 [00:03<00:00, 22.81it/s]\n",
            "test loss at 23 batch 20: 7.024752545970618: 100%|██████████| 21/21 [00:00<00:00, 23.56it/s]\n",
            "training loss at 24 batch 4: 6.773389498314699:   4%|▎         | 3/81 [00:00<00:02, 28.23it/s] "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  7.095367179469591  avg test loss:  7.122051648098505\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 24 batch 80: 6.777714355188132: 100%|██████████| 81/81 [00:03<00:00, 23.81it/s]\n",
            "test loss at 24 batch 20: 6.892400146696212: 100%|██████████| 21/21 [00:00<00:00, 25.63it/s]\n",
            "training loss at 25 batch 3: 6.7779321794710965:   4%|▎         | 3/81 [00:00<00:03, 22.76it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  6.959970145921457  avg test loss:  6.9909317591857905\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 25 batch 80: 6.9333609206508315: 100%|██████████| 81/81 [00:03<00:00, 21.10it/s]\n",
            "test loss at 25 batch 20: 6.765049749462325: 100%|██████████| 21/21 [00:00<00:00, 24.63it/s]\n",
            "training loss at 26 batch 2: 6.859676132953041:   2%|▏         | 2/81 [00:00<00:04, 18.87it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  6.833086594985946  avg test loss:  6.864837655082953\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 26 batch 80: 6.561792912198082: 100%|██████████| 81/81 [00:03<00:00, 23.03it/s]\n",
            "test loss at 26 batch 20: 6.642389076448747: 100%|██████████| 21/21 [00:00<00:00, 35.49it/s]\n",
            "training loss at 27 batch 4: 7.009708135172739:   4%|▎         | 3/81 [00:00<00:02, 29.62it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  6.7090075345001825  avg test loss:  6.743451185199588\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 27 batch 80: 6.286229925012579: 100%|██████████| 81/81 [00:03<00:00, 25.86it/s]\n",
            "test loss at 27 batch 20: 6.524176560701217: 100%|██████████| 21/21 [00:00<00:00, 34.22it/s]\n",
            "training loss at 28 batch 4: 6.635750439008585:   5%|▍         | 4/81 [00:00<00:02, 31.34it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  6.5891184643493865  avg test loss:  6.626540653876016\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 28 batch 80: 6.425523034452829: 100%|██████████| 81/81 [00:03<00:00, 22.44it/s]\n",
            "test loss at 28 batch 20: 6.410228698275384: 100%|██████████| 21/21 [00:00<00:00, 25.02it/s]\n",
            "training loss at 29 batch 4: 6.308886804888749:   4%|▎         | 3/81 [00:00<00:02, 27.52it/s] "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  6.475909932191453  avg test loss:  6.513881233930791\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 29 batch 80: 5.95873407196928: 100%|██████████| 81/81 [00:02<00:00, 28.24it/s]\n",
            "test loss at 29 batch 20: 6.300252403147004: 100%|██████████| 21/21 [00:00<00:00, 33.86it/s]\n",
            "training loss at 30 batch 4: 6.280564069413492:   5%|▍         | 4/81 [00:00<00:02, 29.84it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  6.364172919075976  avg test loss:  6.405177686830678\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 30 batch 80: 5.939187109803169: 100%|██████████| 81/81 [00:02<00:00, 27.12it/s]\n",
            "test loss at 30 batch 20: 6.194171111687582: 100%|██████████| 21/21 [00:00<00:00, 26.23it/s]\n",
            "training loss at 31 batch 2: 6.542085293010835:   4%|▎         | 3/81 [00:00<00:03, 20.92it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  6.258181712038169  avg test loss:  6.300302281063438\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 31 batch 80: 5.92824209859313: 100%|██████████| 81/81 [00:03<00:00, 24.08it/s]\n",
            "test loss at 31 batch 20: 6.0916576557598425: 100%|██████████| 21/21 [00:00<00:00, 31.90it/s]\n",
            "training loss at 32 batch 4: 5.730662693047968:   4%|▎         | 3/81 [00:00<00:02, 29.79it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  6.155680080193656  avg test loss:  6.199027483671239\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 32 batch 80: 5.925349940493122: 100%|██████████| 81/81 [00:03<00:00, 24.86it/s]\n",
            "test loss at 32 batch 20: 5.99259972790019: 100%|██████████| 21/21 [00:00<00:00, 33.32it/s]\n",
            "training loss at 33 batch 4: 6.124382037005836:   5%|▍         | 4/81 [00:00<00:02, 31.41it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  6.057313088580225  avg test loss:  6.101167138219898\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 33 batch 80: 6.22808132227656: 100%|██████████| 81/81 [00:03<00:00, 24.96it/s]\n",
            "test loss at 33 batch 20: 5.896835101602151: 100%|██████████| 21/21 [00:00<00:00, 25.45it/s]\n",
            "training loss at 34 batch 3: 5.9727958443407445:   4%|▎         | 3/81 [00:00<00:03, 21.23it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  5.962526565450132  avg test loss:  6.00655683452135\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 34 batch 80: 6.062866760069635: 100%|██████████| 81/81 [00:03<00:00, 26.15it/s]\n",
            "test loss at 34 batch 20: 5.804277460078605: 100%|██████████| 21/21 [00:00<00:00, 31.97it/s]\n",
            "training loss at 35 batch 2: 5.8548960660671945:   2%|▏         | 2/81 [00:00<00:04, 17.88it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  5.870230771015282  avg test loss:  5.915075218995261\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 35 batch 80: 5.817052821504809: 100%|██████████| 81/81 [00:03<00:00, 23.34it/s]\n",
            "test loss at 35 batch 20: 5.714610796830958: 100%|██████████| 21/21 [00:00<00:00, 27.86it/s]\n",
            "training loss at 36 batch 5: 5.670348744416474:   4%|▎         | 3/81 [00:00<00:02, 29.44it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  5.779348272735504  avg test loss:  5.826499107485154\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 36 batch 80: 5.713752274282119: 100%|██████████| 81/81 [00:02<00:00, 27.66it/s]\n",
            "test loss at 36 batch 20: 5.627808887227664: 100%|██████████| 21/21 [00:00<00:00, 35.66it/s]\n",
            "training loss at 37 batch 3: 5.444130158778495:   2%|▏         | 2/81 [00:00<00:04, 18.21it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  5.6925445246227095  avg test loss:  5.740754911013331\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 37 batch 80: 5.727375794667213: 100%|██████████| 81/81 [00:03<00:00, 24.81it/s]\n",
            "test loss at 37 batch 20: 5.543764927932215: 100%|██████████| 21/21 [00:00<00:00, 27.91it/s]\n",
            "training loss at 38 batch 2: 5.43332552102309:   2%|▏         | 2/81 [00:00<00:04, 19.59it/s] "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  5.609820734448097  avg test loss:  5.657696784933236\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 38 batch 80: 5.931575921730317: 100%|██████████| 81/81 [00:03<00:00, 25.38it/s]\n",
            "test loss at 38 batch 20: 5.462370572032903: 100%|██████████| 21/21 [00:00<00:00, 35.21it/s]\n",
            "training loss at 39 batch 3: 5.596433855680313:   4%|▎         | 3/81 [00:00<00:02, 26.91it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  5.528913924258964  avg test loss:  5.577234459220501\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 39 batch 80: 5.388468264927972: 100%|██████████| 81/81 [00:03<00:00, 26.35it/s]\n",
            "test loss at 39 batch 20: 5.383379705861111: 100%|██████████| 21/21 [00:00<00:00, 33.68it/s]\n",
            "training loss at 40 batch 4: 5.345315213697779:   5%|▍         | 4/81 [00:00<00:02, 30.85it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  5.448085042902004  avg test loss:  5.499184721960144\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 40 batch 80: 5.663248263970445: 100%|██████████| 81/81 [00:03<00:00, 25.81it/s]\n",
            "test loss at 40 batch 20: 5.306839669038307: 100%|██████████| 21/21 [00:00<00:00, 36.68it/s]\n",
            "training loss at 41 batch 4: 5.287752641523453:   4%|▎         | 3/81 [00:00<00:02, 28.93it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  5.373692090960271  avg test loss:  5.423512814523639\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 41 batch 80: 5.285663413550255: 100%|██████████| 81/81 [00:03<00:00, 25.65it/s]\n",
            "test loss at 41 batch 20: 5.232518249958946: 100%|██████████| 21/21 [00:00<00:00, 24.12it/s]\n",
            "training loss at 42 batch 2: 5.433170206783383:   2%|▏         | 2/81 [00:00<00:04, 18.82it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  5.297895298805485  avg test loss:  5.3500655000538035\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 42 batch 80: 4.959264123820088: 100%|██████████| 81/81 [00:03<00:00, 25.33it/s]\n",
            "test loss at 42 batch 20: 5.16043516262361: 100%|██████████| 21/21 [00:00<00:00, 35.57it/s]\n",
            "training loss at 43 batch 4: 5.183465005106018:   4%|▎         | 3/81 [00:00<00:02, 28.02it/s] "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  5.225147987643866  avg test loss:  5.278802649584651\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 43 batch 80: 5.5219688335411: 100%|██████████| 81/81 [00:02<00:00, 28.06it/s]\n",
            "test loss at 43 batch 20: 5.090393011720722: 100%|██████████| 21/21 [00:00<00:00, 23.44it/s]\n",
            "training loss at 44 batch 4: 5.084755240648146:   4%|▎         | 3/81 [00:00<00:02, 27.45it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  5.158128624727961  avg test loss:  5.209596341581207\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 44 batch 80: 5.0374576160985: 100%|██████████| 81/81 [00:02<00:00, 27.54it/s]\n",
            "test loss at 44 batch 20: 5.022407690285151: 100%|██████████| 21/21 [00:00<00:00, 29.75it/s]\n",
            "training loss at 45 batch 3: 5.083935209532162:   4%|▎         | 3/81 [00:00<00:02, 26.64it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  5.08874706883819  avg test loss:  5.14235297171215\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 45 batch 80: 4.702732430471067: 100%|██████████| 81/81 [00:03<00:00, 25.59it/s]\n",
            "test loss at 45 batch 20: 4.956335348266277: 100%|██████████| 21/21 [00:00<00:00, 35.87it/s]\n",
            "training loss at 46 batch 4: 4.740958022310887:   4%|▎         | 3/81 [00:00<00:02, 26.56it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  5.022178362200902  avg test loss:  5.077047493720876\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 46 batch 80: 4.982377654335761: 100%|██████████| 81/81 [00:03<00:00, 26.30it/s]\n",
            "test loss at 46 batch 20: 4.892137695305427: 100%|██████████| 21/21 [00:00<00:00, 34.54it/s]\n",
            "training loss at 47 batch 4: 4.999056063680928:   5%|▍         | 4/81 [00:00<00:02, 30.56it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  4.959530629249224  avg test loss:  5.013557308714687\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 47 batch 80: 4.653565134541606: 100%|██████████| 81/81 [00:02<00:00, 28.72it/s]\n",
            "test loss at 47 batch 20: 4.8297078520073855: 100%|██████████| 21/21 [00:00<00:00, 26.10it/s]\n",
            "training loss at 48 batch 3: 5.040984493726492:   2%|▏         | 2/81 [00:00<00:04, 18.22it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  4.896037340487131  avg test loss:  4.951822225387587\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 48 batch 80: 5.145739584136454: 100%|██████████| 81/81 [00:02<00:00, 27.75it/s]\n",
            "test loss at 48 batch 20: 4.769009739492639: 100%|██████████| 21/21 [00:00<00:00, 35.23it/s]\n",
            "training loss at 49 batch 4: 4.685235319295831:   4%|▎         | 3/81 [00:00<00:02, 27.54it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  4.837940342418676  avg test loss:  4.89177415637723\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 49 batch 80: 5.036760554129701: 100%|██████████| 81/81 [00:03<00:00, 26.93it/s]\n",
            "test loss at 49 batch 20: 4.70994028685927: 100%|██████████| 21/21 [00:00<00:00, 32.58it/s]\n",
            "training loss at 50 batch 2: 5.154904197726273:   2%|▏         | 2/81 [00:00<00:04, 18.35it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  4.779127943845949  avg test loss:  4.833344241388407\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 50 batch 80: 4.486911247638823: 100%|██████████| 81/81 [00:02<00:00, 27.64it/s]\n",
            "test loss at 50 batch 20: 4.652446038183819: 100%|██████████| 21/21 [00:00<00:00, 34.23it/s]\n",
            "training loss at 51 batch 4: 4.724114927035624:   4%|▎         | 3/81 [00:00<00:02, 27.96it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  4.719559607826871  avg test loss:  4.776460806722164\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 51 batch 80: 4.729438455516137: 100%|██████████| 81/81 [00:03<00:00, 24.17it/s]\n",
            "test loss at 51 batch 20: 4.5965018898299475: 100%|██████████| 21/21 [00:00<00:00, 33.81it/s]\n",
            "training loss at 52 batch 4: 4.685418292755724:   4%|▎         | 3/81 [00:00<00:02, 28.90it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  4.6655011453370365  avg test loss:  4.721102922976192\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 52 batch 80: 4.3619536494716: 100%|██████████| 81/81 [00:03<00:00, 26.33it/s]\n",
            "test loss at 52 batch 20: 4.542013649109276: 100%|██████████| 21/21 [00:00<00:00, 31.20it/s]\n",
            "training loss at 53 batch 3: 4.536511295995366:   4%|▎         | 3/81 [00:00<00:03, 24.39it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  4.609782623522474  avg test loss:  4.667179717278138\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 53 batch 80: 4.545033067592193: 100%|██████████| 81/81 [00:03<00:00, 25.70it/s]\n",
            "test loss at 53 batch 20: 4.48897142395237: 100%|██████████| 21/21 [00:00<00:00, 24.43it/s]\n",
            "training loss at 54 batch 4: 4.580434642509799:   4%|▎         | 3/81 [00:00<00:02, 28.81it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  4.558448902085976  avg test loss:  4.61467287168554\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 54 batch 80: 4.624758730149333: 100%|██████████| 81/81 [00:02<00:00, 28.00it/s]\n",
            "test loss at 54 batch 20: 4.437203868437031: 100%|██████████| 21/21 [00:00<00:00, 34.29it/s]\n",
            "training loss at 55 batch 4: 4.3923296351048915:   4%|▎         | 3/81 [00:00<00:02, 28.10it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  4.507179290456236  avg test loss:  4.563469425114967\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 55 batch 80: 4.333583723992637: 100%|██████████| 81/81 [00:03<00:00, 25.99it/s]\n",
            "test loss at 55 batch 20: 4.386810362037501: 100%|██████████| 21/21 [00:00<00:00, 30.38it/s]\n",
            "training loss at 56 batch 4: 4.36891162935879:   4%|▎         | 3/81 [00:00<00:02, 28.68it/s] "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  4.456438786639639  avg test loss:  4.513588614195964\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 56 batch 80: 4.562286321819086: 100%|██████████| 81/81 [00:02<00:00, 27.71it/s]\n",
            "test loss at 56 batch 20: 4.337675456094886: 100%|██████████| 21/21 [00:00<00:00, 34.68it/s]\n",
            "training loss at 57 batch 2: 4.636757814681586:   2%|▏         | 2/81 [00:00<00:04, 19.73it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  4.4085002439877865  avg test loss:  4.464954564595102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 57 batch 80: 4.101569691632568: 100%|██████████| 81/81 [00:03<00:00, 24.80it/s]\n",
            "test loss at 57 batch 20: 4.28978388032957: 100%|██████████| 21/21 [00:00<00:00, 32.18it/s]\n",
            "training loss at 58 batch 3: 4.370201963063705:   2%|▏         | 2/81 [00:00<00:03, 19.99it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  4.35944741216165  avg test loss:  4.4175305104991525\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 58 batch 80: 4.352384416294914: 100%|██████████| 81/81 [00:02<00:00, 27.78it/s]\n",
            "test loss at 58 batch 20: 4.243070963614125: 100%|██████████| 21/21 [00:00<00:00, 25.61it/s]\n",
            "training loss at 59 batch 3: 4.631315764741404:   2%|▏         | 2/81 [00:00<00:04, 19.19it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  4.314500621017114  avg test loss:  4.371288851646197\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 59 batch 80: 4.44940099658806: 100%|██████████| 81/81 [00:03<00:00, 26.04it/s]\n",
            "test loss at 59 batch 20: 4.197506603273161: 100%|██████████| 21/21 [00:00<00:00, 32.81it/s]\n",
            "training loss at 60 batch 4: 4.157369015351503:   4%|▎         | 3/81 [00:00<00:02, 29.62it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  4.269943480514363  avg test loss:  4.326151096209325\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 60 batch 80: 4.399181678098935: 100%|██████████| 81/81 [00:03<00:00, 26.50it/s]\n",
            "test loss at 60 batch 20: 4.153021596272296: 100%|██████████| 21/21 [00:00<00:00, 30.84it/s]\n",
            "training loss at 61 batch 3: 4.337700000344775:   2%|▏         | 2/81 [00:00<00:04, 19.64it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  4.22541159492429  avg test loss:  4.282103786111729\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 61 batch 80: 4.065185457377765: 100%|██████████| 81/81 [00:03<00:00, 22.82it/s]\n",
            "test loss at 61 batch 20: 4.109617027520945: 100%|██████████| 21/21 [00:00<00:00, 32.23it/s]\n",
            "training loss at 62 batch 4: 4.082970131406616:   4%|▎         | 3/81 [00:00<00:02, 27.08it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  4.181255868086466  avg test loss:  4.239101798547873\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 62 batch 80: 4.149238594127596: 100%|██████████| 81/81 [00:03<00:00, 26.57it/s]\n",
            "test loss at 62 batch 20: 4.067192468395724: 100%|██████████| 21/21 [00:00<00:00, 25.24it/s]\n",
            "training loss at 63 batch 3: 3.9119755081943803:   4%|▎         | 3/81 [00:00<00:03, 22.27it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  4.139789582540976  avg test loss:  4.197108148326668\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 63 batch 80: 4.492291310804875: 100%|██████████| 81/81 [00:03<00:00, 25.53it/s]\n",
            "test loss at 63 batch 20: 4.025782004767197: 100%|██████████| 21/21 [00:00<00:00, 31.31it/s]\n",
            "training loss at 64 batch 3: 3.908622834974028:   4%|▎         | 3/81 [00:00<00:03, 21.84it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  4.100201561525966  avg test loss:  4.156109557605942\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 64 batch 80: 3.870383490951763: 100%|██████████| 81/81 [00:03<00:00, 24.76it/s]\n",
            "test loss at 64 batch 20: 3.9853606902565604: 100%|██████████| 21/21 [00:00<00:00, 32.87it/s]\n",
            "training loss at 65 batch 3: 3.9834218466035596:   2%|▏         | 2/81 [00:00<00:03, 19.91it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  4.057692926186795  avg test loss:  4.116057475432685\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 65 batch 80: 3.777785745007866: 100%|██████████| 81/81 [00:03<00:00, 25.92it/s]\n",
            "test loss at 65 batch 20: 3.9458453598543235: 100%|██████████| 21/21 [00:00<00:00, 32.37it/s]\n",
            "training loss at 66 batch 4: 3.843343110669242:   4%|▎         | 3/81 [00:00<00:02, 28.88it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  4.018300480884164  avg test loss:  4.076943272145444\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 66 batch 80: 4.181161009839795: 100%|██████████| 81/81 [00:03<00:00, 26.13it/s]\n",
            "test loss at 66 batch 20: 3.907238874343535: 100%|██████████| 21/21 [00:00<00:00, 36.14it/s]\n",
            "training loss at 67 batch 4: 3.860066532835177:   4%|▎         | 3/81 [00:00<00:02, 29.53it/s] "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  3.9818066910826393  avg test loss:  4.038718208857977\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 67 batch 80: 3.9854558709205032: 100%|██████████| 81/81 [00:03<00:00, 22.67it/s]\n",
            "test loss at 67 batch 20: 3.869486391527756: 100%|██████████| 21/21 [00:00<00:00, 32.26it/s]\n",
            "training loss at 68 batch 3: 3.8234496238674494:   2%|▏         | 2/81 [00:00<00:04, 18.28it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  3.9437674279732953  avg test loss:  4.001342696046474\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 68 batch 80: 3.845111879493537: 100%|██████████| 81/81 [00:02<00:00, 27.78it/s]\n",
            "test loss at 68 batch 20: 3.83260903750751: 100%|██████████| 21/21 [00:00<00:00, 26.64it/s]\n",
            "training loss at 69 batch 2: 3.8630934922085336:   2%|▏         | 2/81 [00:00<00:04, 19.11it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  3.906913798763936  avg test loss:  3.964813283371246\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 69 batch 80: 3.8803261528360036: 100%|██████████| 81/81 [00:03<00:00, 24.54it/s]\n",
            "test loss at 69 batch 20: 3.7965612180914476: 100%|██████████| 21/21 [00:00<00:00, 34.19it/s]\n",
            "training loss at 70 batch 4: 3.630994604906184:   5%|▍         | 4/81 [00:00<00:02, 28.78it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  3.8715635775357997  avg test loss:  3.929097640371999\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 70 batch 80: 3.7126340050290016: 100%|██████████| 81/81 [00:02<00:00, 28.02it/s]\n",
            "test loss at 70 batch 20: 3.7612569654380974: 100%|██████████| 21/21 [00:00<00:00, 31.63it/s]\n",
            "training loss at 71 batch 2: 3.6467084371100498:   2%|▏         | 2/81 [00:00<00:04, 17.80it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  3.8368441502812733  avg test loss:  3.894150160803009\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 71 batch 80: 3.8170066801855693: 100%|██████████| 81/81 [00:03<00:00, 25.63it/s]\n",
            "test loss at 71 batch 20: 3.7267592806510006: 100%|██████████| 21/21 [00:00<00:00, 33.40it/s]\n",
            "training loss at 72 batch 3: 3.7323639861050686:   4%|▎         | 3/81 [00:00<00:03, 23.56it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  3.802950449730607  avg test loss:  3.8599851514434573\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 72 batch 80: 3.583512291407694: 100%|██████████| 81/81 [00:02<00:00, 28.24it/s]\n",
            "test loss at 72 batch 20: 3.692969751045666: 100%|██████████| 21/21 [00:00<00:00, 26.05it/s]\n",
            "training loss at 73 batch 4: 3.6359051610107342:   4%|▎         | 3/81 [00:00<00:03, 25.78it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  3.768391217785616  avg test loss:  3.8265442018118345\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 73 batch 80: 3.717705616295973: 100%|██████████| 81/81 [00:03<00:00, 25.89it/s]\n",
            "test loss at 73 batch 20: 3.6599698234667124: 100%|██████████| 21/21 [00:00<00:00, 30.44it/s]\n",
            "training loss at 74 batch 4: 3.730371056301342:   4%|▎         | 3/81 [00:00<00:02, 26.67it/s] "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  3.7364337948912345  avg test loss:  3.793834530407718\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 74 batch 80: 3.694908818792607: 100%|██████████| 81/81 [00:03<00:00, 26.42it/s]\n",
            "test loss at 74 batch 20: 3.627643793173351: 100%|██████████| 21/21 [00:00<00:00, 31.83it/s]\n",
            "training loss at 75 batch 4: 3.6072824586922874:   4%|▎         | 3/81 [00:00<00:03, 25.35it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  3.704663971514485  avg test loss:  3.7618161140739765\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 75 batch 80: 3.753417195170473: 100%|██████████| 81/81 [00:03<00:00, 24.57it/s]\n",
            "test loss at 75 batch 20: 3.595976980866497: 100%|██████████| 21/21 [00:00<00:00, 33.31it/s]\n",
            "training loss at 76 batch 4: 3.359946646859099:   4%|▎         | 3/81 [00:00<00:02, 28.79it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  3.6737186105957647  avg test loss:  3.7304657950480213\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 76 batch 80: 3.660252715223278: 100%|██████████| 81/81 [00:03<00:00, 24.06it/s]\n",
            "test loss at 76 batch 20: 3.5649929737564015: 100%|██████████| 21/21 [00:00<00:00, 30.93it/s]\n",
            "training loss at 77 batch 3: 3.6493163631779164:   2%|▏         | 2/81 [00:00<00:04, 19.49it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  3.6427613144492232  avg test loss:  3.6997806453512383\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 77 batch 80: 3.512105092156903: 100%|██████████| 81/81 [00:03<00:00, 23.85it/s]\n",
            "test loss at 77 batch 20: 3.53465672709467: 100%|██████████| 21/21 [00:00<00:00, 28.00it/s]\n",
            "training loss at 78 batch 4: 3.60604162347711:   4%|▎         | 3/81 [00:00<00:03, 25.60it/s] "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  3.612013606584216  avg test loss:  3.6697289233610895\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 78 batch 80: 3.5170015076267203: 100%|██████████| 81/81 [00:03<00:00, 24.30it/s]\n",
            "test loss at 78 batch 20: 3.504899737040103: 100%|██████████| 21/21 [00:00<00:00, 24.42it/s]\n",
            "training loss at 79 batch 2: 3.6186511770443244:   4%|▎         | 3/81 [00:00<00:03, 20.66it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  3.5835290918327645  avg test loss:  3.6402930931982524\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 79 batch 80: 3.5203884737160642: 100%|██████████| 81/81 [00:03<00:00, 22.08it/s]\n",
            "test loss at 79 batch 20: 3.4757864357120405: 100%|██████████| 21/21 [00:00<00:00, 31.48it/s]\n",
            "training loss at 80 batch 4: 3.6271641480044994:   4%|▎         | 3/81 [00:00<00:02, 26.38it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  3.554592571822816  avg test loss:  3.611460336248119\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 80 batch 80: 3.8367508973136872: 100%|██████████| 81/81 [00:02<00:00, 27.11it/s]\n",
            "test loss at 80 batch 20: 3.447279397760243: 100%|██████████| 21/21 [00:00<00:00, 33.97it/s]\n",
            "training loss at 81 batch 4: 3.614393947360879:   4%|▎         | 3/81 [00:00<00:02, 28.13it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  3.5281690430914914  avg test loss:  3.5832092938947064\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 81 batch 80: 3.6158298979763885: 100%|██████████| 81/81 [00:03<00:00, 23.54it/s]\n",
            "test loss at 81 batch 20: 3.41934799899327: 100%|██████████| 21/21 [00:00<00:00, 33.99it/s]\n",
            "training loss at 82 batch 4: 3.401489472972193:   4%|▎         | 3/81 [00:00<00:02, 27.17it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  3.499534364468149  avg test loss:  3.555520029312118\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 82 batch 80: 3.64724985498421: 100%|██████████| 81/81 [00:03<00:00, 22.72it/s]\n",
            "test loss at 82 batch 20: 3.391937549744826: 100%|██████████| 21/21 [00:00<00:00, 36.05it/s]\n",
            "training loss at 83 batch 3: 3.348600958224318:   4%|▎         | 3/81 [00:00<00:03, 23.70it/s] "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  3.4729607088895253  avg test loss:  3.528385744912318\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 83 batch 80: 3.2281476851414714: 100%|██████████| 81/81 [00:03<00:00, 24.53it/s]\n",
            "test loss at 83 batch 20: 3.36507708967607: 100%|██████████| 21/21 [00:00<00:00, 25.04it/s]\n",
            "training loss at 84 batch 3: 3.7003934152615643:   4%|▎         | 3/81 [00:00<00:02, 27.39it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  3.444762160481014  avg test loss:  3.501789312316773\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 84 batch 80: 3.330764763520127: 100%|██████████| 81/81 [00:03<00:00, 24.06it/s]\n",
            "test loss at 84 batch 20: 3.338773259878463: 100%|██████████| 21/21 [00:00<00:00, 34.80it/s]\n",
            "training loss at 85 batch 4: 3.3705353368434303:   4%|▎         | 3/81 [00:00<00:02, 27.42it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  3.419363556042572  avg test loss:  3.4757239461852047\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 85 batch 80: 3.36641289714021: 100%|██████████| 81/81 [00:03<00:00, 26.95it/s]\n",
            "test loss at 85 batch 20: 3.3129478495456053: 100%|██████████| 21/21 [00:00<00:00, 31.87it/s]\n",
            "training loss at 86 batch 4: 3.5703592398839334:   4%|▎         | 3/81 [00:00<00:03, 25.68it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  3.3940777606753088  avg test loss:  3.4501567568566185\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 86 batch 80: 3.2720973815647056: 100%|██████████| 81/81 [00:03<00:00, 22.39it/s]\n",
            "test loss at 86 batch 20: 3.2876502405307884: 100%|██████████| 21/21 [00:00<00:00, 34.34it/s]\n",
            "training loss at 87 batch 4: 3.403186313767524:   4%|▎         | 3/81 [00:00<00:02, 28.71it/s] "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  3.368962959156004  avg test loss:  3.4250889920256133\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 87 batch 80: 3.303322897003162: 100%|██████████| 81/81 [00:03<00:00, 24.39it/s]\n",
            "test loss at 87 batch 20: 3.2628198879699597: 100%|██████████| 21/21 [00:00<00:00, 25.83it/s]\n",
            "training loss at 88 batch 4: 3.283954998173944:   4%|▎         | 3/81 [00:00<00:03, 25.98it/s] "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  3.3445889495398644  avg test loss:  3.400499456662459\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 88 batch 80: 3.036895826196504: 100%|██████████| 81/81 [00:03<00:00, 26.23it/s]\n",
            "test loss at 88 batch 20: 3.2384656418851345: 100%|██████████| 21/21 [00:00<00:00, 35.43it/s]\n",
            "training loss at 89 batch 3: 3.246512430352598:   4%|▎         | 3/81 [00:00<00:03, 24.76it/s] "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  3.3198932058662187  avg test loss:  3.3763826556562835\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 89 batch 80: 3.2345811359362338: 100%|██████████| 81/81 [00:03<00:00, 26.37it/s]\n",
            "test loss at 89 batch 20: 3.21458239535061: 100%|██████████| 21/21 [00:00<00:00, 22.94it/s]\n",
            "training loss at 90 batch 2: 3.3905868304830222:   2%|▏         | 2/81 [00:00<00:04, 18.40it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  3.297370881391477  avg test loss:  3.352722964978829\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 90 batch 80: 3.0210179858787365: 100%|██████████| 81/81 [00:03<00:00, 21.77it/s]\n",
            "test loss at 90 batch 20: 3.191138456588845: 100%|██████████| 21/21 [00:00<00:00, 32.02it/s]\n",
            "training loss at 91 batch 3: 3.3071408500671846:   4%|▎         | 3/81 [00:00<00:03, 24.36it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  3.273223371053728  avg test loss:  3.329502231933957\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 91 batch 80: 3.120452678255389: 100%|██████████| 81/81 [00:03<00:00, 26.38it/s]\n",
            "test loss at 91 batch 20: 3.168129127330991: 100%|██████████| 21/21 [00:00<00:00, 31.84it/s]\n",
            "training loss at 92 batch 3: 3.2344153511068336:   4%|▎         | 3/81 [00:00<00:02, 28.92it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  3.251208249409739  avg test loss:  3.3067182286186503\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 92 batch 80: 3.300846735451916: 100%|██████████| 81/81 [00:02<00:00, 27.25it/s]\n",
            "test loss at 92 batch 20: 3.1455493823598744: 100%|██████████| 21/21 [00:00<00:00, 33.98it/s]\n",
            "training loss at 93 batch 4: 3.1735505307780705:   4%|▎         | 3/81 [00:00<00:02, 27.86it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  3.229863841049574  avg test loss:  3.284347482749492\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 93 batch 80: 3.181405091752174: 100%|██████████| 81/81 [00:03<00:00, 25.52it/s]\n",
            "test loss at 93 batch 20: 3.123402555193272: 100%|██████████| 21/21 [00:00<00:00, 28.36it/s]\n",
            "training loss at 94 batch 2: 3.3210796037396286:   2%|▏         | 2/81 [00:00<00:04, 18.39it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  3.2077728228557554  avg test loss:  3.262397776501058\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 94 batch 80: 3.1811854300900393: 100%|██████████| 81/81 [00:03<00:00, 21.27it/s]\n",
            "test loss at 94 batch 20: 3.1016319496459404: 100%|██████████| 21/21 [00:00<00:00, 34.03it/s]\n",
            "training loss at 95 batch 3: 3.15198572763322:   4%|▎         | 3/81 [00:00<00:02, 28.15it/s] "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  3.1863424480327622  avg test loss:  3.240839955622168\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 95 batch 80: 3.132210992315835: 100%|██████████| 81/81 [00:03<00:00, 23.66it/s]\n",
            "test loss at 95 batch 20: 3.0802628163432524: 100%|██████████| 21/21 [00:00<00:00, 32.35it/s]\n",
            "training loss at 96 batch 4: 3.3120022449577897:   4%|▎         | 3/81 [00:00<00:03, 24.63it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  3.1648801563450037  avg test loss:  3.2196714614364663\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 96 batch 80: 3.4263465811719804: 100%|██████████| 81/81 [00:02<00:00, 27.11it/s]\n",
            "test loss at 96 batch 20: 3.0592740198254207: 100%|██████████| 21/21 [00:00<00:00, 24.60it/s]\n",
            "training loss at 97 batch 3: 3.161303925444532:   2%|▏         | 2/81 [00:00<00:04, 19.09it/s] "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  3.1459711987156713  avg test loss:  3.1988892673498426\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 97 batch 80: 3.116533888717061: 100%|██████████| 81/81 [00:03<00:00, 24.00it/s]\n",
            "test loss at 97 batch 20: 3.0386542212279593: 100%|██████████| 21/21 [00:00<00:00, 24.49it/s]\n",
            "training loss at 98 batch 3: 3.1827308654334066:   4%|▎         | 3/81 [00:00<00:03, 24.73it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  3.1244936456262224  avg test loss:  3.178460127553627\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 98 batch 80: 3.1319141997183935: 100%|██████████| 81/81 [00:03<00:00, 23.12it/s]\n",
            "test loss at 98 batch 20: 3.01843039566905: 100%|██████████| 21/21 [00:00<00:00, 33.05it/s]\n",
            "training loss at 99 batch 4: 2.930438965297586:   4%|▎         | 3/81 [00:00<00:03, 24.30it/s] "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  3.1046763957978865  avg test loss:  3.1584058072061705\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 99 batch 80: 3.2483666836398544: 100%|██████████| 81/81 [00:03<00:00, 26.12it/s]\n",
            "test loss at 99 batch 20: 2.9985472054187325: 100%|██████████| 21/21 [00:00<00:00, 22.36it/s]\n",
            "training loss at 100 batch 2: 2.9864813953006135:   2%|▏         | 2/81 [00:00<00:04, 17.22it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  3.085708308721915  avg test loss:  3.138701802463115\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 100 batch 80: 3.0017259231590487: 100%|██████████| 81/81 [00:03<00:00, 20.85it/s]\n",
            "test loss at 100 batch 20: 2.9790158687399844: 100%|██████████| 21/21 [00:00<00:00, 25.96it/s]\n",
            "training loss at 101 batch 4: 2.724232781843916:   4%|▎         | 3/81 [00:00<00:02, 27.41it/s] "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  3.065621121895183  avg test loss:  3.119346748827413\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 101 batch 80: 2.9576786709560405: 100%|██████████| 81/81 [00:03<00:00, 24.22it/s]\n",
            "test loss at 101 batch 20: 2.9598122962771543: 100%|██████████| 21/21 [00:00<00:00, 29.16it/s]\n",
            "training loss at 102 batch 4: 3.0714283799135105:   4%|▎         | 3/81 [00:00<00:03, 25.26it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  3.046428360808861  avg test loss:  3.1003240063386492\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 102 batch 80: 3.086983978482909: 100%|██████████| 81/81 [00:02<00:00, 27.54it/s]\n",
            "test loss at 102 batch 20: 2.9409634380714778: 100%|██████████| 21/21 [00:00<00:00, 24.92it/s]\n",
            "training loss at 103 batch 3: 2.8960629492257595:   4%|▎         | 3/81 [00:00<00:03, 22.31it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  3.028602283064275  avg test loss:  3.08164294610485\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 103 batch 80: 2.9825490375051174: 100%|██████████| 81/81 [00:03<00:00, 20.53it/s]\n",
            "test loss at 103 batch 20: 2.9224237755234816: 100%|██████████| 21/21 [00:00<00:00, 29.07it/s]\n",
            "training loss at 104 batch 4: 2.8108218917793146:   4%|▎         | 3/81 [00:00<00:03, 25.35it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  3.010171104625542  avg test loss:  3.063266953358748\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 104 batch 80: 2.970591191654088: 100%|██████████| 81/81 [00:03<00:00, 21.42it/s]\n",
            "test loss at 104 batch 20: 2.9041916990230026: 100%|██████████| 21/21 [00:00<00:00, 33.29it/s]\n",
            "training loss at 105 batch 3: 3.062295429613817:   4%|▎         | 3/81 [00:00<00:03, 23.47it/s] "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.9921309319746796  avg test loss:  3.045210668939859\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 105 batch 80: 2.935893906117504: 100%|██████████| 81/81 [00:03<00:00, 23.49it/s]\n",
            "test loss at 105 batch 20: 2.886282623925164: 100%|██████████| 21/21 [00:00<00:00, 29.95it/s]\n",
            "training loss at 106 batch 3: 3.008415842454025:   2%|▏         | 2/81 [00:00<00:03, 19.84it/s] "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.9752091783399512  avg test loss:  3.027463271574998\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 106 batch 80: 2.8076027152146144: 100%|██████████| 81/81 [00:03<00:00, 24.06it/s]\n",
            "test loss at 106 batch 20: 2.868678228380455: 100%|██████████| 21/21 [00:00<00:00, 24.25it/s]\n",
            "training loss at 107 batch 2: 3.0277309959587133:   2%|▏         | 2/81 [00:00<00:04, 19.46it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.957017097299782  avg test loss:  3.0100105408214644\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 107 batch 80: 2.9376850143014464: 100%|██████████| 81/81 [00:03<00:00, 21.87it/s]\n",
            "test loss at 107 batch 20: 2.85133674362119: 100%|██████████| 21/21 [00:00<00:00, 23.57it/s]\n",
            "training loss at 108 batch 3: 2.7408226402698124:   4%|▎         | 3/81 [00:00<00:02, 27.57it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.940801599969751  avg test loss:  2.9928406645862076\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 108 batch 80: 2.9851561266137177: 100%|██████████| 81/81 [00:03<00:00, 25.14it/s]\n",
            "test loss at 108 batch 20: 2.834302510998502: 100%|██████████| 21/21 [00:00<00:00, 32.08it/s]\n",
            "training loss at 109 batch 4: 2.8065809724696265:   4%|▎         | 3/81 [00:00<00:02, 27.86it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.924126905665409  avg test loss:  2.975953492862651\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 109 batch 80: 2.9070639516623107: 100%|██████████| 81/81 [00:03<00:00, 25.03it/s]\n",
            "test loss at 109 batch 20: 2.817554084876727: 100%|██████████| 21/21 [00:00<00:00, 34.46it/s]\n",
            "training loss at 110 batch 3: 3.1260194191364477:   4%|▎         | 3/81 [00:00<00:02, 28.77it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.9077846794270656  avg test loss:  2.9593485330055556\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 110 batch 80: 2.6854889998088827: 100%|██████████| 81/81 [00:03<00:00, 25.14it/s]\n",
            "test loss at 110 batch 20: 2.8010938803346628: 100%|██████████| 21/21 [00:00<00:00, 32.26it/s]\n",
            "training loss at 111 batch 4: 2.7601794928814423:   4%|▎         | 3/81 [00:00<00:02, 29.06it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.890254037543084  avg test loss:  2.943015101653447\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 111 batch 80: 3.008530122909998: 100%|██████████| 81/81 [00:03<00:00, 24.24it/s]\n",
            "test loss at 111 batch 20: 2.7848986833550287: 100%|██████████| 21/21 [00:00<00:00, 29.49it/s]\n",
            "training loss at 112 batch 2: 2.821431579214861:   2%|▏         | 2/81 [00:00<00:05, 15.33it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.876009655345975  avg test loss:  2.9269616398706293\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 112 batch 80: 3.088121160960932: 100%|██████████| 81/81 [00:03<00:00, 22.63it/s]\n",
            "test loss at 112 batch 20: 2.768945962532891: 100%|██████████| 21/21 [00:00<00:00, 28.02it/s]\n",
            "training loss at 113 batch 3: 2.714780436600402:   4%|▎         | 3/81 [00:00<00:02, 28.17it/s] "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.860415902767459  avg test loss:  2.9111443379654367\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 113 batch 80: 2.7104974753738107: 100%|██████████| 81/81 [00:03<00:00, 22.22it/s]\n",
            "test loss at 113 batch 20: 2.753280905909375: 100%|██████████| 21/21 [00:00<00:00, 29.94it/s]\n",
            "training loss at 114 batch 4: 2.935902462397908:   4%|▎         | 3/81 [00:00<00:03, 25.58it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.8438344347457125  avg test loss:  2.895586363738473\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 114 batch 80: 2.818860146184999: 100%|██████████| 81/81 [00:03<00:00, 26.76it/s]\n",
            "test loss at 114 batch 20: 2.7378391549194996: 100%|██████████| 21/21 [00:00<00:00, 28.88it/s]\n",
            "training loss at 115 batch 4: 2.887941326329866:   4%|▎         | 3/81 [00:00<00:02, 29.17it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.8296634025684786  avg test loss:  2.880280782684155\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 115 batch 80: 2.8412012630383683: 100%|██████████| 81/81 [00:03<00:00, 24.77it/s]\n",
            "test loss at 115 batch 20: 2.722645848892503: 100%|██████████| 21/21 [00:00<00:00, 30.74it/s]\n",
            "training loss at 116 batch 4: 2.881564837691297:   4%|▎         | 3/81 [00:00<00:02, 27.27it/s] "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.814558518681374  avg test loss:  2.8652153572547703\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 116 batch 80: 2.506723295652135: 100%|██████████| 81/81 [00:03<00:00, 24.02it/s]\n",
            "test loss at 116 batch 20: 2.7077020353688668: 100%|██████████| 21/21 [00:00<00:00, 30.78it/s]\n",
            "training loss at 117 batch 4: 2.808776425998727:   4%|▎         | 3/81 [00:00<00:02, 26.81it/s] "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.798580087883979  avg test loss:  2.850388863565994\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 117 batch 80: 2.9972553828390502: 100%|██████████| 81/81 [00:03<00:00, 26.55it/s]\n",
            "test loss at 117 batch 20: 2.6930192258669097: 100%|██████████| 21/21 [00:00<00:00, 27.07it/s]\n",
            "training loss at 118 batch 3: 2.8048488928047077:   4%|▎         | 3/81 [00:00<00:03, 22.38it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.7863648414219715  avg test loss:  2.8358039436371\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 118 batch 80: 2.9716815353936643: 100%|██████████| 81/81 [00:03<00:00, 23.71it/s]\n",
            "test loss at 118 batch 20: 2.678538799562567: 100%|██████████| 21/21 [00:00<00:00, 24.90it/s]\n",
            "training loss at 119 batch 2: 2.8078860470557583:   2%|▏         | 2/81 [00:00<00:04, 18.73it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.771960206464197  avg test loss:  2.8214304224150606\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 119 batch 80: 2.719733116774309: 100%|██████████| 81/81 [00:03<00:00, 23.78it/s]\n",
            "test loss at 119 batch 20: 2.6642832627392954: 100%|██████████| 21/21 [00:00<00:00, 32.32it/s]\n",
            "training loss at 120 batch 4: 2.8008162029160015:   4%|▎         | 3/81 [00:00<00:02, 28.19it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.756934259237632  avg test loss:  2.807279043790334\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 120 batch 80: 2.681251516553947: 100%|██████████| 81/81 [00:03<00:00, 23.85it/s]\n",
            "test loss at 120 batch 20: 2.650243948383033: 100%|██████████| 21/21 [00:00<00:00, 29.92it/s]\n",
            "training loss at 121 batch 4: 2.6199057775303625:   4%|▎         | 3/81 [00:00<00:03, 25.72it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.7431700160900467  avg test loss:  2.7933467961897955\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 121 batch 80: 2.661200810510377: 100%|██████████| 81/81 [00:03<00:00, 26.01it/s]\n",
            "test loss at 121 batch 20: 2.6364340422757615: 100%|██████████| 21/21 [00:00<00:00, 30.97it/s]\n",
            "training loss at 122 batch 4: 2.756407025615743:   4%|▎         | 3/81 [00:00<00:02, 27.01it/s] "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.72958076085154  avg test loss:  2.779634693127595\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 122 batch 80: 2.8760258866949506: 100%|██████████| 81/81 [00:03<00:00, 23.10it/s]\n",
            "test loss at 122 batch 20: 2.622839276335154: 100%|██████████| 21/21 [00:00<00:00, 31.10it/s]\n",
            "training loss at 123 batch 3: 2.6130718900526815:   2%|▏         | 2/81 [00:00<00:04, 19.17it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.7169952033093927  avg test loss:  2.7661255107506233\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 123 batch 80: 2.7637966652423365: 100%|██████████| 81/81 [00:03<00:00, 24.35it/s]\n",
            "test loss at 123 batch 20: 2.609433757172092: 100%|██████████| 21/21 [00:00<00:00, 29.35it/s]\n",
            "training loss at 124 batch 4: 2.569740183876319:   4%|▎         | 3/81 [00:00<00:02, 28.16it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.703550112369148  avg test loss:  2.752819114341295\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 124 batch 80: 2.7892797414354424: 100%|██████████| 81/81 [00:03<00:00, 24.23it/s]\n",
            "test loss at 124 batch 20: 2.5962426832098497: 100%|██████████| 21/21 [00:00<00:00, 33.44it/s]\n",
            "training loss at 125 batch 4: 2.5938771948074364:   4%|▎         | 3/81 [00:00<00:02, 28.17it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.6908224180131004  avg test loss:  2.7397063380922284\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 125 batch 80: 2.7470993554481904: 100%|██████████| 81/81 [00:03<00:00, 25.90it/s]\n",
            "test loss at 125 batch 20: 2.583245443604231: 100%|██████████| 21/21 [00:00<00:00, 28.98it/s]\n",
            "training loss at 126 batch 4: 2.692000512445715:   4%|▎         | 3/81 [00:00<00:02, 29.40it/s] "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.6782770868708345  avg test loss:  2.72678977376009\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 126 batch 80: 2.7103420622377863: 100%|██████████| 81/81 [00:03<00:00, 25.96it/s]\n",
            "test loss at 126 batch 20: 2.570435621688512: 100%|██████████| 21/21 [00:00<00:00, 31.44it/s]\n",
            "training loss at 127 batch 3: 2.6817878482394897:   2%|▏         | 2/81 [00:00<00:05, 15.59it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.6652663973290465  avg test loss:  2.7140666075480544\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 127 batch 80: 2.5503814791907784: 100%|██████████| 81/81 [00:03<00:00, 24.53it/s]\n",
            "test loss at 127 batch 20: 2.5578317177228964: 100%|██████████| 21/21 [00:00<00:00, 26.36it/s]\n",
            "training loss at 128 batch 2: 2.479571276507773:   2%|▏         | 2/81 [00:00<00:04, 19.65it/s] "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.652368092478842  avg test loss:  2.701528417230234\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 128 batch 80: 2.5656763633573716: 100%|██████████| 81/81 [00:03<00:00, 22.46it/s]\n",
            "test loss at 128 batch 20: 2.5454077095055325: 100%|██████████| 21/21 [00:00<00:00, 25.15it/s]\n",
            "training loss at 129 batch 3: 2.8539424248679075:   4%|▎         | 3/81 [00:00<00:03, 24.70it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.6404805063432355  avg test loss:  2.689178848503761\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 129 batch 80: 2.435617818416131: 100%|██████████| 81/81 [00:03<00:00, 23.12it/s]\n",
            "test loss at 129 batch 20: 2.5331561939363874: 100%|██████████| 21/21 [00:00<00:00, 29.81it/s]\n",
            "training loss at 130 batch 2: 2.533827630371763:   2%|▏         | 2/81 [00:00<00:04, 17.98it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.627847214115069  avg test loss:  2.677007185938968\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 130 batch 80: 2.6783089393383532: 100%|██████████| 81/81 [00:03<00:00, 23.33it/s]\n",
            "test loss at 130 batch 20: 2.521095910893041: 100%|██████████| 21/21 [00:00<00:00, 25.14it/s]\n",
            "training loss at 131 batch 2: 2.5878515582438255:   2%|▏         | 2/81 [00:00<00:04, 16.89it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.616848930185525  avg test loss:  2.6650183440444026\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 131 batch 80: 2.609391971070027: 100%|██████████| 81/81 [00:03<00:00, 24.33it/s]\n",
            "test loss at 131 batch 20: 2.509212368119264: 100%|██████████| 21/21 [00:00<00:00, 22.61it/s]\n",
            "training loss at 132 batch 2: 2.798676107359125:   4%|▎         | 3/81 [00:00<00:03, 20.60it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.6050362724283502  avg test loss:  2.6531923182730153\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 132 batch 80: 2.4818332012654225: 100%|██████████| 81/81 [00:03<00:00, 22.21it/s]\n",
            "test loss at 132 batch 20: 2.497483580359682: 100%|██████████| 21/21 [00:00<00:00, 33.01it/s]\n",
            "training loss at 133 batch 4: 2.6107155644081184:   4%|▎         | 3/81 [00:00<00:02, 26.59it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.5936585719207677  avg test loss:  2.6415357094758543\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 133 batch 80: 2.5905052496197487: 100%|██████████| 81/81 [00:03<00:00, 23.65it/s]\n",
            "test loss at 133 batch 20: 2.485953129118019: 100%|██████████| 21/21 [00:00<00:00, 30.74it/s]\n",
            "training loss at 134 batch 4: 2.6252563090069576:   4%|▎         | 3/81 [00:00<00:02, 27.61it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.582624129725934  avg test loss:  2.630046357163129\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 134 batch 80: 2.410700240012379: 100%|██████████| 81/81 [00:03<00:00, 23.87it/s]\n",
            "test loss at 134 batch 20: 2.4745866125998495: 100%|██████████| 21/21 [00:00<00:00, 31.93it/s]\n",
            "training loss at 135 batch 4: 2.6712645813854623:   4%|▎         | 3/81 [00:00<00:02, 28.61it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.570486687873858  avg test loss:  2.6187142955272793\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 135 batch 80: 2.4533308973900927: 100%|██████████| 81/81 [00:03<00:00, 26.59it/s]\n",
            "test loss at 135 batch 20: 2.4633672458984734: 100%|██████████| 21/21 [00:00<00:00, 31.87it/s]\n",
            "training loss at 136 batch 2: 2.556444874049028:   4%|▎         | 3/81 [00:00<00:03, 21.64it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.559650047658173  avg test loss:  2.607543679306881\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 136 batch 80: 2.3128570084302558: 100%|██████████| 81/81 [00:03<00:00, 26.17it/s]\n",
            "test loss at 136 batch 20: 2.4523144901558465: 100%|██████████| 21/21 [00:00<00:00, 31.24it/s]\n",
            "training loss at 137 batch 4: 2.4718214978804376:   4%|▎         | 3/81 [00:00<00:02, 28.95it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.5483638502992556  avg test loss:  2.596532180868892\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 137 batch 80: 2.636950931927886: 100%|██████████| 81/81 [00:03<00:00, 26.84it/s]\n",
            "test loss at 137 batch 20: 2.441420194999055: 100%|██████████| 21/21 [00:00<00:00, 33.76it/s]\n",
            "training loss at 138 batch 4: 2.3842522289570476:   4%|▎         | 3/81 [00:00<00:02, 26.72it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.5390218068761774  avg test loss:  2.5856773426067217\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 138 batch 80: 2.6075985592242636: 100%|██████████| 81/81 [00:03<00:00, 22.72it/s]\n",
            "test loss at 138 batch 20: 2.4306654936447276: 100%|██████████| 21/21 [00:00<00:00, 22.47it/s]\n",
            "training loss at 139 batch 3: 2.609797283010656:   4%|▎         | 3/81 [00:00<00:03, 22.35it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.5290190844747378  avg test loss:  2.5749616401170607\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 139 batch 80: 2.3880266588618504: 100%|██████████| 81/81 [00:03<00:00, 21.81it/s]\n",
            "test loss at 139 batch 20: 2.4200689295778246: 100%|██████████| 21/21 [00:00<00:00, 27.64it/s]\n",
            "training loss at 140 batch 2: 2.435664066280381:   2%|▏         | 2/81 [00:00<00:04, 19.16it/s] "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.517291834494223  avg test loss:  2.5643972710599874\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 140 batch 80: 2.49107748354147: 100%|██████████| 81/81 [00:03<00:00, 23.88it/s]\n",
            "test loss at 140 batch 20: 2.409632125716093: 100%|██████████| 21/21 [00:00<00:00, 22.74it/s]\n",
            "training loss at 141 batch 3: 2.476842951166421:   2%|▏         | 2/81 [00:00<00:04, 19.18it/s] "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.507432601259872  avg test loss:  2.5539767134601337\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 141 batch 80: 2.4494690938898347: 100%|██████████| 81/81 [00:03<00:00, 24.77it/s]\n",
            "test loss at 141 batch 20: 2.3993204113294397: 100%|██████████| 21/21 [00:00<00:00, 23.52it/s]\n",
            "training loss at 142 batch 3: 2.4639017213617613:   4%|▎         | 3/81 [00:00<00:03, 23.92it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.496935016348765  avg test loss:  2.5436957317957396\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 142 batch 80: 2.5384859886653235: 100%|██████████| 81/81 [00:03<00:00, 23.59it/s]\n",
            "test loss at 142 batch 20: 2.389152900579051: 100%|██████████| 21/21 [00:00<00:00, 24.41it/s]\n",
            "training loss at 143 batch 2: 2.515218597285791:   2%|▏         | 2/81 [00:00<00:04, 19.29it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.4876885229803634  avg test loss:  2.5335541685318517\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 143 batch 80: 2.33973814446197: 100%|██████████| 81/81 [00:03<00:00, 23.72it/s]\n",
            "test loss at 143 batch 20: 2.379134395463879: 100%|██████████| 21/21 [00:00<00:00, 33.69it/s]\n",
            "training loss at 144 batch 3: 2.509134775291089:   4%|▎         | 3/81 [00:00<00:04, 19.28it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.4766734598889584  avg test loss:  2.52354856380143\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 144 batch 80: 2.3611541281473882: 100%|██████████| 81/81 [00:03<00:00, 24.10it/s]\n",
            "test loss at 144 batch 20: 2.3692466094590054: 100%|██████████| 21/21 [00:00<00:00, 27.19it/s]\n",
            "training loss at 145 batch 2: 2.6085555723578806:   2%|▏         | 2/81 [00:00<00:04, 19.60it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.467306574495522  avg test loss:  2.5136781591689243\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 145 batch 80: 2.386515374405519: 100%|██████████| 81/81 [00:03<00:00, 25.56it/s]\n",
            "test loss at 145 batch 20: 2.3594775414174314: 100%|██████████| 21/21 [00:00<00:00, 31.76it/s]\n",
            "training loss at 146 batch 3: 2.468433891685982:   4%|▎         | 3/81 [00:00<00:02, 27.62it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.4578253700103443  avg test loss:  2.503936192451801\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 146 batch 80: 2.501217181172126: 100%|██████████| 81/81 [00:03<00:00, 26.15it/s]\n",
            "test loss at 146 batch 20: 2.34985974608076: 100%|██████████| 21/21 [00:00<00:00, 29.88it/s]\n",
            "training loss at 147 batch 4: 2.3572313012596604:   4%|▎         | 3/81 [00:00<00:02, 26.31it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.4489215259483768  avg test loss:  2.494320538332669\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 147 batch 80: 2.2463800428737857: 100%|██████████| 81/81 [00:03<00:00, 23.68it/s]\n",
            "test loss at 147 batch 20: 2.3403600151993227: 100%|██████████| 21/21 [00:00<00:00, 29.65it/s]\n",
            "training loss at 148 batch 2: 2.439348934098697:   4%|▎         | 3/81 [00:00<00:03, 19.70it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.4385504918192944  avg test loss:  2.4848308975258724\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 148 batch 80: 2.4824047551361295: 100%|██████████| 81/81 [00:03<00:00, 22.89it/s]\n",
            "test loss at 148 batch 20: 2.3309976664152465: 100%|██████████| 21/21 [00:00<00:00, 30.10it/s]\n",
            "training loss at 149 batch 4: 2.4558349826464356:   4%|▎         | 3/81 [00:00<00:02, 26.30it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.4303569986732074  avg test loss:  2.475466993872208\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 149 batch 80: 2.4247092239174552: 100%|██████████| 81/81 [00:03<00:00, 22.59it/s]\n",
            "test loss at 149 batch 20: 2.3217571019691237: 100%|██████████| 21/21 [00:00<00:00, 22.88it/s]\n",
            "training loss at 150 batch 3: 2.3533437292436687:   4%|▎         | 3/81 [00:00<00:03, 25.47it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.421478802464363  avg test loss:  2.466222142333218\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 150 batch 80: 2.366953799089644: 100%|██████████| 81/81 [00:03<00:00, 23.33it/s]\n",
            "test loss at 150 batch 20: 2.312629090393274: 100%|██████████| 21/21 [00:00<00:00, 32.79it/s]\n",
            "training loss at 151 batch 4: 2.3744565596569065:   4%|▎         | 3/81 [00:00<00:02, 27.94it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.41185409154474  avg test loss:  2.4570967058608084\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 151 batch 80: 2.301695211512287: 100%|██████████| 81/81 [00:03<00:00, 26.08it/s]\n",
            "test loss at 151 batch 20: 2.303642595559111: 100%|██████████| 21/21 [00:00<00:00, 31.90it/s]\n",
            "training loss at 152 batch 2: 2.4414365383895005:   2%|▏         | 2/81 [00:00<00:04, 19.61it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.4033880519143582  avg test loss:  2.4480869311181856\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 152 batch 80: 2.5413848254387204: 100%|██████████| 81/81 [00:03<00:00, 22.88it/s]\n",
            "test loss at 152 batch 20: 2.294772747239103: 100%|██████████| 21/21 [00:00<00:00, 31.59it/s]\n",
            "training loss at 153 batch 2: 2.3649610263146035:   2%|▏         | 2/81 [00:00<00:04, 17.81it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.3951100297039796  avg test loss:  2.439197882852819\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 153 batch 80: 2.4269456706411088: 100%|██████████| 81/81 [00:03<00:00, 25.31it/s]\n",
            "test loss at 153 batch 20: 2.2860098257411265: 100%|██████████| 21/21 [00:00<00:00, 32.08it/s]\n",
            "training loss at 154 batch 2: 2.4783817874988876:   2%|▏         | 2/81 [00:00<00:04, 19.35it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.3864106270666086  avg test loss:  2.430413612155532\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 154 batch 80: 2.431957352823922: 100%|██████████| 81/81 [00:03<00:00, 24.33it/s]\n",
            "test loss at 154 batch 20: 2.277357214581455: 100%|██████████| 21/21 [00:00<00:00, 31.26it/s]\n",
            "training loss at 155 batch 2: 2.5669113321792008:   1%|          | 1/81 [00:00<00:08,  9.98it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.377750511443786  avg test loss:  2.4217424725256174\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 155 batch 80: 2.3581648691064787: 100%|██████████| 81/81 [00:03<00:00, 22.70it/s]\n",
            "test loss at 155 batch 20: 2.268824908357902: 100%|██████████| 21/21 [00:00<00:00, 28.28it/s]\n",
            "training loss at 156 batch 2: 2.3662878452824323:   2%|▏         | 2/81 [00:00<00:04, 16.30it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.3690537825780194  avg test loss:  2.413179361058564\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 156 batch 80: 2.331862326620047: 100%|██████████| 81/81 [00:03<00:00, 22.35it/s]\n",
            "test loss at 156 batch 20: 2.260398272533992: 100%|██████████| 21/21 [00:00<00:00, 33.61it/s]\n",
            "training loss at 157 batch 2: 2.1930957732035017:   2%|▏         | 2/81 [00:00<00:04, 18.34it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.3605555765190744  avg test loss:  2.40472151825902\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 157 batch 80: 2.543386605889299: 100%|██████████| 81/81 [00:03<00:00, 22.32it/s]\n",
            "test loss at 157 batch 20: 2.252081124100441: 100%|██████████| 21/21 [00:00<00:00, 31.89it/s]\n",
            "training loss at 158 batch 4: 2.4339908854221415:   4%|▎         | 3/81 [00:00<00:02, 27.42it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.353363761014508  avg test loss:  2.396374082831833\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 158 batch 80: 2.2893149905453853: 100%|██████████| 81/81 [00:03<00:00, 25.40it/s]\n",
            "test loss at 158 batch 20: 2.243864994428495: 100%|██████████| 21/21 [00:00<00:00, 33.89it/s]\n",
            "training loss at 159 batch 4: 2.4558480892253547:   4%|▎         | 3/81 [00:00<00:02, 28.49it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.344039777825214  avg test loss:  2.388126200719428\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 159 batch 80: 2.284418252681589: 100%|██████████| 81/81 [00:03<00:00, 26.02it/s]\n",
            "test loss at 159 batch 20: 2.235755628430141: 100%|██████████| 21/21 [00:00<00:00, 31.65it/s]\n",
            "training loss at 160 batch 4: 2.27341428893101:   4%|▎         | 3/81 [00:00<00:02, 27.04it/s]  "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.336034533140548  avg test loss:  2.379979960660278\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 160 batch 80: 2.361962490582956: 100%|██████████| 81/81 [00:03<00:00, 22.84it/s]\n",
            "test loss at 160 batch 20: 2.2277596830686317: 100%|██████████| 21/21 [00:00<00:00, 27.15it/s]\n",
            "training loss at 161 batch 1: 2.3237367657477885:   1%|          | 1/81 [00:00<00:08,  9.79it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.328570384399137  avg test loss:  2.3719334515650194\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 161 batch 80: 2.3329793685464852: 100%|██████████| 81/81 [00:03<00:00, 21.59it/s]\n",
            "test loss at 161 batch 20: 2.2198516910175705: 100%|██████████| 21/21 [00:00<00:00, 32.33it/s]\n",
            "training loss at 162 batch 4: 2.278253446275214:   4%|▎         | 3/81 [00:00<00:02, 28.44it/s] "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.3208001001893983  avg test loss:  2.3639808178791206\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 162 batch 80: 2.2736492181594543: 100%|██████████| 81/81 [00:03<00:00, 26.02it/s]\n",
            "test loss at 162 batch 20: 2.2120396543044953: 100%|██████████| 21/21 [00:00<00:00, 32.54it/s]\n",
            "training loss at 163 batch 4: 2.2014750014211204:   4%|▎         | 3/81 [00:00<00:02, 27.53it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.3127948146460406  avg test loss:  2.356126263101368\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 163 batch 80: 2.2626745268908053: 100%|██████████| 81/81 [00:03<00:00, 23.83it/s]\n",
            "test loss at 163 batch 20: 2.2043297372405917: 100%|██████████| 21/21 [00:00<00:00, 30.88it/s]\n",
            "training loss at 164 batch 4: 2.331581191787344:   4%|▎         | 3/81 [00:00<00:02, 28.24it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.3051992683409357  avg test loss:  2.348364670494694\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 164 batch 80: 2.3170528465825226: 100%|██████████| 81/81 [00:03<00:00, 25.40it/s]\n",
            "test loss at 164 batch 20: 2.196703528988471: 100%|██████████| 21/21 [00:00<00:00, 31.94it/s]\n",
            "training loss at 165 batch 4: 2.42803711794104:   4%|▎         | 3/81 [00:00<00:02, 27.24it/s] "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.2978312464606705  avg test loss:  2.340694327709193\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 165 batch 80: 2.2020906936069595: 100%|██████████| 81/81 [00:03<00:00, 22.74it/s]\n",
            "test loss at 165 batch 20: 2.1891797751227964: 100%|██████████| 21/21 [00:00<00:00, 23.65it/s]\n",
            "training loss at 166 batch 3: 2.2079208399816705:   2%|▏         | 2/81 [00:00<00:04, 18.91it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.2900513995654217  avg test loss:  2.3331184891059515\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 166 batch 80: 2.19320852715647: 100%|██████████| 81/81 [00:03<00:00, 24.46it/s]\n",
            "test loss at 166 batch 20: 2.1817561318955043: 100%|██████████| 21/21 [00:00<00:00, 24.19it/s]\n",
            "training loss at 167 batch 1: 2.2509173176750474:   1%|          | 1/81 [00:00<00:08,  9.18it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.2825649265929133  avg test loss:  2.325630086006956\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 167 batch 80: 2.346592126172102: 100%|██████████| 81/81 [00:03<00:00, 23.80it/s]\n",
            "test loss at 167 batch 20: 2.174412182664492: 100%|██████████| 21/21 [00:00<00:00, 33.57it/s]\n",
            "training loss at 168 batch 3: 2.2249111985111707:   4%|▎         | 3/81 [00:00<00:03, 24.91it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.275856488448887  avg test loss:  2.318232784846078\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 168 batch 80: 2.222877437516471: 100%|██████████| 81/81 [00:03<00:00, 24.69it/s]\n",
            "test loss at 168 batch 20: 2.1671553002179045: 100%|██████████| 21/21 [00:00<00:00, 32.51it/s]\n",
            "training loss at 169 batch 3: 2.3405441920268437:   4%|▎         | 3/81 [00:00<00:03, 22.34it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.2682712264520384  avg test loss:  2.3109138076598956\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 169 batch 80: 2.459088722837124: 100%|██████████| 81/81 [00:03<00:00, 25.62it/s]\n",
            "test loss at 169 batch 20: 2.159980651648054: 100%|██████████| 21/21 [00:00<00:00, 32.08it/s]\n",
            "training loss at 170 batch 4: 2.368300326960827:   4%|▎         | 3/81 [00:00<00:02, 27.26it/s] "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.262695881514905  avg test loss:  2.303683661247049\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 170 batch 80: 2.257781075361157: 100%|██████████| 81/81 [00:03<00:00, 24.57it/s]\n",
            "test loss at 170 batch 20: 2.1528899584627554: 100%|██████████| 21/21 [00:00<00:00, 32.14it/s]\n",
            "training loss at 171 batch 4: 2.2111535657680363:   4%|▎         | 3/81 [00:00<00:02, 27.36it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.254350098499179  avg test loss:  2.2965337314991765\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 171 batch 80: 2.272113305823659: 100%|██████████| 81/81 [00:03<00:00, 24.07it/s]\n",
            "test loss at 171 batch 20: 2.1458911584810934: 100%|██████████| 21/21 [00:00<00:00, 27.82it/s]\n",
            "training loss at 172 batch 4: 2.3145173593455506:   4%|▎         | 3/81 [00:00<00:03, 24.08it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.2477531786048437  avg test loss:  2.289466320318565\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 172 batch 80: 2.19896522694219: 100%|██████████| 81/81 [00:03<00:00, 22.25it/s]\n",
            "test loss at 172 batch 20: 2.138977697309966: 100%|██████████| 21/21 [00:00<00:00, 22.92it/s]\n",
            "training loss at 173 batch 1: 2.3170418701603426:   1%|          | 1/81 [00:00<00:08,  9.86it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.240879626489281  avg test loss:  2.2824842188796683\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 173 batch 80: 2.18176697045105: 100%|██████████| 81/81 [00:03<00:00, 23.39it/s]\n",
            "test loss at 173 batch 20: 2.132152031794731: 100%|██████████| 21/21 [00:00<00:00, 31.53it/s]\n",
            "training loss at 174 batch 4: 2.308705061952262:   4%|▎         | 3/81 [00:00<00:02, 27.09it/s] "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.233716000977475  avg test loss:  2.275580837945504\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 174 batch 80: 2.0857353021724503: 100%|██████████| 81/81 [00:03<00:00, 24.83it/s]\n",
            "test loss at 174 batch 20: 2.125391881709183: 100%|██████████| 21/21 [00:00<00:00, 25.90it/s]\n",
            "training loss at 175 batch 4: 2.30581765604626:   4%|▎         | 3/81 [00:00<00:02, 27.02it/s]  "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.226799588971283  avg test loss:  2.2687528236103125\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 175 batch 80: 2.218615146132406: 100%|██████████| 81/81 [00:03<00:00, 25.38it/s]\n",
            "test loss at 175 batch 20: 2.1187287685735856: 100%|██████████| 21/21 [00:00<00:00, 26.35it/s]\n",
            "training loss at 176 batch 3: 2.166726048468044:   2%|▏         | 2/81 [00:00<00:04, 18.95it/s] "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.220811654856763  avg test loss:  2.2620091844234245\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 176 batch 80: 2.0975376069201586: 100%|██████████| 81/81 [00:03<00:00, 24.08it/s]\n",
            "test loss at 176 batch 20: 2.1121326545047587: 100%|██████████| 21/21 [00:00<00:00, 23.29it/s]\n",
            "training loss at 177 batch 2: 2.426611092318056:   4%|▎         | 3/81 [00:00<00:03, 19.59it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.213373085479589  avg test loss:  2.2553314853423827\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 177 batch 80: 2.2773319023524903: 100%|██████████| 81/81 [00:03<00:00, 21.53it/s]\n",
            "test loss at 177 batch 20: 2.1056302786660357: 100%|██████████| 21/21 [00:00<00:00, 30.39it/s]\n",
            "training loss at 178 batch 4: 2.1075785061240118:   4%|▎         | 3/81 [00:00<00:02, 27.00it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.207847546516202  avg test loss:  2.2487351909694264\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 178 batch 80: 2.1964866550030595: 100%|██████████| 81/81 [00:03<00:00, 24.86it/s]\n",
            "test loss at 178 batch 20: 2.0991881191680384: 100%|██████████| 21/21 [00:00<00:00, 26.83it/s]\n",
            "training loss at 179 batch 1: 2.1357274197724734:   1%|          | 1/81 [00:00<00:08,  9.66it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.2011382939236848  avg test loss:  2.2422075093027365\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 179 batch 80: 2.0134198942632064: 100%|██████████| 81/81 [00:03<00:00, 25.05it/s]\n",
            "test loss at 179 batch 20: 2.092810641485241: 100%|██████████| 21/21 [00:00<00:00, 31.91it/s]\n",
            "training loss at 180 batch 4: 2.1447818534976957:   4%|▎         | 3/81 [00:00<00:02, 28.10it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.1943153688067305  avg test loss:  2.235752740253689\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 180 batch 80: 2.1383421120930284: 100%|██████████| 81/81 [00:03<00:00, 22.84it/s]\n",
            "test loss at 180 batch 20: 2.0865269958359782: 100%|██████████| 21/21 [00:00<00:00, 30.50it/s]\n",
            "training loss at 181 batch 4: 2.4226745815796975:   4%|▎         | 3/81 [00:00<00:02, 28.68it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.188775752851196  avg test loss:  2.2293706032059415\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 181 batch 80: 2.1196229948170187: 100%|██████████| 81/81 [00:03<00:00, 24.37it/s]\n",
            "test loss at 181 batch 20: 2.0802989494168904: 100%|██████████| 21/21 [00:00<00:00, 31.35it/s]\n",
            "training loss at 182 batch 3: 2.063520627914416:   4%|▎         | 3/81 [00:00<00:03, 23.14it/s] "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.1821998190716285  avg test loss:  2.2230567792246485\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 182 batch 80: 2.1000550155588877: 100%|██████████| 81/81 [00:03<00:00, 23.80it/s]\n",
            "test loss at 182 batch 20: 2.074158719533124: 100%|██████████| 21/21 [00:00<00:00, 33.30it/s]\n",
            "training loss at 183 batch 4: 2.010375892095459:   4%|▎         | 3/81 [00:00<00:02, 27.74it/s] "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.176020442517691  avg test loss:  2.216813291981007\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 183 batch 80: 2.2048434206801937: 100%|██████████| 81/81 [00:03<00:00, 24.32it/s]\n",
            "test loss at 183 batch 20: 2.0680738700525647: 100%|██████████| 21/21 [00:00<00:00, 32.67it/s]\n",
            "training loss at 184 batch 4: 2.1363613265554666:   4%|▎         | 3/81 [00:00<00:02, 27.22it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.170561873569629  avg test loss:  2.2106369071232583\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 184 batch 80: 2.267526757656973: 100%|██████████| 81/81 [00:03<00:00, 24.25it/s]\n",
            "test loss at 184 batch 20: 2.0620730079165046: 100%|██████████| 21/21 [00:00<00:00, 29.86it/s]\n",
            "training loss at 185 batch 2: 2.103103412813097:   2%|▏         | 2/81 [00:00<00:05, 14.89it/s] "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.1648518586626113  avg test loss:  2.204527372677787\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 185 batch 80: 2.203211603199246: 100%|██████████| 81/81 [00:03<00:00, 22.69it/s]\n",
            "test loss at 185 batch 20: 2.0561213949305244: 100%|██████████| 21/21 [00:00<00:00, 31.87it/s]\n",
            "training loss at 186 batch 3: 2.228532816729569:   2%|▏         | 2/81 [00:00<00:04, 19.44it/s] "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.1583771510422918  avg test loss:  2.1984781654508345\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 186 batch 80: 2.1377636142927305: 100%|██████████| 81/81 [00:03<00:00, 26.26it/s]\n",
            "test loss at 186 batch 20: 2.05025180001866: 100%|██████████| 21/21 [00:00<00:00, 33.68it/s]\n",
            "training loss at 187 batch 3: 2.2076453295089156:   4%|▎         | 3/81 [00:00<00:03, 23.98it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.1527185823514876  avg test loss:  2.1924923407774966\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 187 batch 80: 2.0550742013696772: 100%|██████████| 81/81 [00:03<00:00, 22.25it/s]\n",
            "test loss at 187 batch 20: 2.044447014865723: 100%|██████████| 21/21 [00:00<00:00, 33.54it/s]\n",
            "training loss at 188 batch 3: 2.0878637907455806:   4%|▎         | 3/81 [00:00<00:03, 25.20it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.146387665133052  avg test loss:  2.1865722036755093\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 188 batch 80: 2.129480064918858: 100%|██████████| 81/81 [00:03<00:00, 24.42it/s]\n",
            "test loss at 188 batch 20: 2.038711943215219: 100%|██████████| 21/21 [00:00<00:00, 31.54it/s]\n",
            "training loss at 189 batch 4: 2.0382461746239957:   4%|▎         | 3/81 [00:00<00:02, 28.80it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.1409617143087463  avg test loss:  2.1807212960473583\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 189 batch 80: 2.306119956092445: 100%|██████████| 81/81 [00:03<00:00, 25.85it/s]\n",
            "test loss at 189 batch 20: 2.0330311311365237: 100%|██████████| 21/21 [00:00<00:00, 34.42it/s]\n",
            "training loss at 190 batch 4: 2.135563100533393:   4%|▎         | 3/81 [00:00<00:02, 28.34it/s] "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.136065080265849  avg test loss:  2.174927898521359\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 190 batch 80: 2.121939195830898: 100%|██████████| 81/81 [00:03<00:00, 25.86it/s]\n",
            "test loss at 190 batch 20: 2.0274216694140867: 100%|██████████| 21/21 [00:00<00:00, 25.77it/s]\n",
            "training loss at 191 batch 2: 2.27838519058517:   2%|▏         | 2/81 [00:00<00:04, 15.90it/s] "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.1298794783109787  avg test loss:  2.169189929060672\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 191 batch 80: 2.3001952490163737: 100%|██████████| 81/81 [00:03<00:00, 24.05it/s]\n",
            "test loss at 191 batch 20: 2.02185405000177: 100%|██████████| 21/21 [00:00<00:00, 27.44it/s]\n",
            "training loss at 192 batch 3: 2.1047206305671784:   2%|▏         | 2/81 [00:00<00:03, 19.89it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.125219962805723  avg test loss:  2.163513447865161\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 192 batch 80: 2.1205646083319163: 100%|██████████| 81/81 [00:03<00:00, 25.06it/s]\n",
            "test loss at 192 batch 20: 2.016345022059463: 100%|██████████| 21/21 [00:00<00:00, 34.00it/s]\n",
            "training loss at 193 batch 3: 2.136866035804707:   4%|▎         | 3/81 [00:00<00:03, 25.68it/s] "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.1189097652158457  avg test loss:  2.1578948648464165\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 193 batch 80: 2.183031480403469: 100%|██████████| 81/81 [00:03<00:00, 23.89it/s]\n",
            "test loss at 193 batch 20: 2.010902619720581: 100%|██████████| 21/21 [00:00<00:00, 33.27it/s]\n",
            "training loss at 194 batch 3: 2.133442691823051:   4%|▎         | 3/81 [00:00<00:02, 27.92it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.113542992162179  avg test loss:  2.1523371780330818\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 194 batch 80: 2.0643360538497624: 100%|██████████| 81/81 [00:03<00:00, 23.95it/s]\n",
            "test loss at 194 batch 20: 2.0055303047225608: 100%|██████████| 21/21 [00:00<00:00, 33.33it/s]\n",
            "training loss at 195 batch 3: 2.0149864818422696:   4%|▎         | 3/81 [00:00<00:03, 23.11it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.1077756567598054  avg test loss:  2.146832736221941\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 195 batch 80: 2.2565143454878487: 100%|██████████| 81/81 [00:03<00:00, 24.19it/s]\n",
            "test loss at 195 batch 20: 2.0002132300654307: 100%|██████████| 21/21 [00:00<00:00, 33.65it/s]\n",
            "training loss at 196 batch 4: 2.207260336382991:   4%|▎         | 3/81 [00:00<00:02, 26.79it/s] "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.102917250528184  avg test loss:  2.141388170965143\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 196 batch 80: 2.1423994742015684: 100%|██████████| 81/81 [00:03<00:00, 22.51it/s]\n",
            "test loss at 196 batch 20: 1.994951049775362: 100%|██████████| 21/21 [00:00<00:00, 29.78it/s]\n",
            "training loss at 197 batch 2: 2.0655046834170467:   2%|▏         | 2/81 [00:00<00:05, 14.70it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.097412860130052  avg test loss:  2.1359996305712055\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 197 batch 80: 2.0404600692880166: 100%|██████████| 81/81 [00:03<00:00, 24.82it/s]\n",
            "test loss at 197 batch 20: 1.989743359247741: 100%|██████████| 21/21 [00:00<00:00, 32.65it/s]\n",
            "training loss at 198 batch 4: 2.0959705930289085:   4%|▎         | 3/81 [00:00<00:02, 27.10it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.0919767849145563  avg test loss:  2.1306617573699036\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 198 batch 80: 2.086573322631726: 100%|██████████| 81/81 [00:03<00:00, 25.78it/s]\n",
            "test loss at 198 batch 20: 1.9846040352915975: 100%|██████████| 21/21 [00:00<00:00, 28.08it/s]\n",
            "training loss at 199 batch 3: 1.928817036935899:   4%|▎         | 3/81 [00:00<00:03, 23.63it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.0871627670663058  avg test loss:  2.125377666295697\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 199 batch 80: 2.0695759713060022: 100%|██████████| 81/81 [00:03<00:00, 23.74it/s]\n",
            "test loss at 199 batch 20: 1.9794949384792593: 100%|██████████| 21/21 [00:00<00:00, 26.12it/s]\n",
            "training loss at 200 batch 4: 2.1253683893875093:   4%|▎         | 3/81 [00:00<00:02, 26.94it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.0818370393983803  avg test loss:  2.1201504391683312\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 200 batch 80: 2.0179295504848844: 100%|██████████| 81/81 [00:03<00:00, 20.71it/s]\n",
            "test loss at 200 batch 20: 1.9744531784911599: 100%|██████████| 21/21 [00:00<00:00, 33.06it/s]\n",
            "training loss at 201 batch 4: 2.1182397685321157:   4%|▎         | 3/81 [00:00<00:03, 25.54it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.076452081096404  avg test loss:  2.1149709182692145\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 201 batch 80: 2.1238019996901785: 100%|██████████| 81/81 [00:04<00:00, 20.12it/s]\n",
            "test loss at 201 batch 20: 1.9694603573162341: 100%|██████████| 21/21 [00:00<00:00, 31.42it/s]\n",
            "training loss at 202 batch 4: 2.0530513899596956:   4%|▎         | 3/81 [00:00<00:02, 27.40it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.0722173465266693  avg test loss:  2.109843756756114\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 202 batch 80: 2.2083403234585006: 100%|██████████| 81/81 [00:03<00:00, 23.04it/s]\n",
            "test loss at 202 batch 20: 1.9645255672397053: 100%|██████████| 21/21 [00:00<00:00, 34.07it/s]\n",
            "training loss at 203 batch 2: 2.18893838418472:   2%|▏         | 2/81 [00:00<00:05, 14.89it/s]  "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.06730323842078  avg test loss:  2.1047635815019694\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 203 batch 80: 2.070209608196868: 100%|██████████| 81/81 [00:03<00:00, 24.99it/s]\n",
            "test loss at 203 batch 20: 1.9596398447720538: 100%|██████████| 21/21 [00:00<00:00, 32.53it/s]\n",
            "training loss at 204 batch 4: 2.0270291824388167:   4%|▎         | 3/81 [00:00<00:02, 26.82it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.061998786186167  avg test loss:  2.0997353776821917\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 204 batch 80: 2.2457571512421266: 100%|██████████| 81/81 [00:03<00:00, 24.40it/s]\n",
            "test loss at 204 batch 20: 1.9548050684162266: 100%|██████████| 21/21 [00:00<00:00, 29.96it/s]\n",
            "training loss at 205 batch 4: 2.0125635824074792:   4%|▎         | 3/81 [00:00<00:03, 25.99it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.057860763002719  avg test loss:  2.0947573437494773\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 205 batch 80: 1.887518176701043: 100%|██████████| 81/81 [00:03<00:00, 23.25it/s]\n",
            "test loss at 205 batch 20: 1.9500159901772898: 100%|██████████| 21/21 [00:00<00:00, 31.76it/s]\n",
            "training loss at 206 batch 3: 2.0319365020082203:   4%|▎         | 3/81 [00:00<00:02, 28.89it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.0513301766292167  avg test loss:  2.0898213655200433\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 206 batch 80: 2.173697548837907: 100%|██████████| 81/81 [00:03<00:00, 24.22it/s]\n",
            "test loss at 206 batch 20: 1.9452827589136168: 100%|██████████| 21/21 [00:00<00:00, 33.07it/s]\n",
            "training loss at 207 batch 3: 2.1019726004484256:   4%|▎         | 3/81 [00:00<00:02, 27.63it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.0480170285995913  avg test loss:  2.08494319948409\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 207 batch 80: 1.9931291350824516: 100%|██████████| 81/81 [00:03<00:00, 24.75it/s]\n",
            "test loss at 207 batch 20: 1.9405913298368394: 100%|██████████| 21/21 [00:00<00:00, 29.92it/s]\n",
            "training loss at 208 batch 3: 2.102447636550167:   4%|▎         | 3/81 [00:00<00:02, 26.58it/s] "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.042462977607086  avg test loss:  2.080104976785453\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 208 batch 80: 2.074684132650785: 100%|██████████| 81/81 [00:03<00:00, 24.48it/s]\n",
            "test loss at 208 batch 20: 1.9359603958543314: 100%|██████████| 21/21 [00:00<00:00, 29.82it/s]\n",
            "training loss at 209 batch 2: 1.9749969788753796:   2%|▏         | 2/81 [00:00<00:05, 15.21it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.0384299895008104  avg test loss:  2.0753154417291135\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 209 batch 80: 2.0430280908897007: 100%|██████████| 81/81 [00:04<00:00, 19.38it/s]\n",
            "test loss at 209 batch 20: 1.931358070242279: 100%|██████████| 21/21 [00:00<00:00, 31.00it/s]\n",
            "training loss at 210 batch 4: 1.943468413223357:   4%|▎         | 3/81 [00:00<00:02, 26.62it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.0336898967446086  avg test loss:  2.070566646180843\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 210 batch 80: 2.008962099335387: 100%|██████████| 81/81 [00:03<00:00, 23.69it/s]\n",
            "test loss at 210 batch 20: 1.926808257404464: 100%|██████████| 21/21 [00:00<00:00, 23.70it/s]\n",
            "training loss at 211 batch 3: 2.0509596438076:   4%|▎         | 3/81 [00:00<00:02, 26.10it/s]  "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.0288215490448533  avg test loss:  2.065868805902069\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 211 batch 80: 2.1547897192317516: 100%|██████████| 81/81 [00:03<00:00, 22.54it/s]\n",
            "test loss at 211 batch 20: 1.9223053832181762: 100%|██████████| 21/21 [00:00<00:00, 27.17it/s]\n",
            "training loss at 212 batch 4: 1.9819254099840113:   4%|▎         | 3/81 [00:00<00:02, 28.24it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.0247663684372035  avg test loss:  2.0612137976654568\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 212 batch 80: 2.0476574862873798: 100%|██████████| 81/81 [00:03<00:00, 26.25it/s]\n",
            "test loss at 212 batch 20: 1.9178402746941157: 100%|██████████| 21/21 [00:00<00:00, 32.02it/s]\n",
            "training loss at 213 batch 4: 1.9962473417817528:   4%|▎         | 3/81 [00:00<00:02, 27.82it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.0196528799074596  avg test loss:  2.0566016095638684\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 213 batch 80: 1.957687136134893: 100%|██████████| 81/81 [00:03<00:00, 23.47it/s]\n",
            "test loss at 213 batch 20: 1.9134195014963338: 100%|██████████| 21/21 [00:00<00:00, 28.03it/s]\n",
            "training loss at 214 batch 2: 1.8526042697595217:   4%|▎         | 3/81 [00:00<00:03, 21.13it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.015033579735785  avg test loss:  2.052028535799851\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 214 batch 80: 1.84002698214707: 100%|██████████| 81/81 [00:03<00:00, 23.41it/s]\n",
            "test loss at 214 batch 20: 1.9090559838265784: 100%|██████████| 21/21 [00:00<00:00, 23.98it/s]\n",
            "training loss at 215 batch 1: 1.9266233018056504:   2%|▏         | 2/81 [00:00<00:05, 15.12it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.0100574045419695  avg test loss:  2.0475036154966326\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 215 batch 80: 1.9548908378257737: 100%|██████████| 81/81 [00:03<00:00, 23.36it/s]\n",
            "test loss at 215 batch 20: 1.9047362277218913: 100%|██████████| 21/21 [00:00<00:00, 33.34it/s]\n",
            "training loss at 216 batch 2: 2.0129456145769367:   2%|▏         | 2/81 [00:00<00:04, 17.62it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.0066443569303147  avg test loss:  2.043022591892302\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 216 batch 80: 1.888462858492125: 100%|██████████| 81/81 [00:03<00:00, 25.18it/s]\n",
            "test loss at 216 batch 20: 1.900444707860902: 100%|██████████| 21/21 [00:00<00:00, 25.77it/s]\n",
            "training loss at 217 batch 3: 1.96447608427157:   2%|▏         | 2/81 [00:00<00:04, 19.06it/s] "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  2.0014937599596068  avg test loss:  2.038579170162003\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 217 batch 80: 1.8801306504374515: 100%|██████████| 81/81 [00:03<00:00, 20.66it/s]\n",
            "test loss at 217 batch 20: 1.8962017823622335: 100%|██████████| 21/21 [00:00<00:00, 30.33it/s]\n",
            "training loss at 218 batch 4: 1.9933358881667984:   4%|▎         | 3/81 [00:00<00:02, 26.81it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  1.9969838971388214  avg test loss:  2.034175634818373\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 218 batch 80: 1.9535291121190097: 100%|██████████| 81/81 [00:03<00:00, 25.68it/s]\n",
            "test loss at 218 batch 20: 1.8920130946487286: 100%|██████████| 21/21 [00:00<00:00, 31.89it/s]\n",
            "training loss at 219 batch 4: 1.9997397198997025:   4%|▎         | 3/81 [00:00<00:02, 27.21it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  1.9932842561323552  avg test loss:  2.0298157033856676\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 219 batch 80: 2.0942797635260444: 100%|██████████| 81/81 [00:03<00:00, 23.71it/s]\n",
            "test loss at 219 batch 20: 1.8878489624178212: 100%|██████████| 21/21 [00:00<00:00, 27.41it/s]\n",
            "training loss at 220 batch 2: 2.0273014943242615:   2%|▏         | 2/81 [00:00<00:04, 17.74it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  1.9897755176900143  avg test loss:  2.0254909750946672\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 220 batch 80: 1.9398156580926535: 100%|██████████| 81/81 [00:03<00:00, 20.72it/s]\n",
            "test loss at 220 batch 20: 1.8837299817576154: 100%|██████████| 21/21 [00:00<00:00, 25.03it/s]\n",
            "training loss at 221 batch 2: 1.9142419488339948:   1%|          | 1/81 [00:00<00:08,  9.61it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg training loss:  1.9851192191443332  avg test loss:  2.0212048553974764\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at 221 batch 80: 1.9664361874581078: 100%|██████████| 81/81 [00:03<00:00, 24.01it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " EARLY STOP!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDDH7LUVcHPT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "06fb06de-4315-4eb8-fd11-593253c59751"
      },
      "source": [
        "from matplotlib import pyplot as plt\r\n",
        "plt.plot(epoches_list, train_losses, label='train loss')\r\n",
        "plt.plot(epoches_list, test_losses, label='test loss')\r\n",
        "plt.legend(loc=\"upper right\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f90627fc690>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HswBJCQl6_B"
      },
      "source": [
        "np_items = model.item_factors.weight.data.cpu().numpy()\r\n",
        "np_users = model.user_factors.weight.data.cpu().numpy()\r\n",
        "np_item_bias = model.item_biases.weight.data.cpu().numpy()\r\n",
        "np_user_bias = model.user_biases.weight.data.cpu().numpy()"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KU9Mkql6rs_V"
      },
      "source": [
        "Выберем фильм Pulp Fiction  и с помощью косинусной меры определим 10 наиболее похожих на него."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqOdjMhdq6nP"
      },
      "source": [
        "def cos_dist(a, b):\r\n",
        "  return a.dot(b)/(np.linalg.norm(a)*np.linalg.norm(b))"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QH1rsbUqr5B6"
      },
      "source": [
        "movie_id = movie_dict[296]\r\n",
        "similarity_dict = {}\r\n",
        "for movie, id in zip(np_items, movie_dict.keys()):\r\n",
        "  similarity_dict[id] = cos_dist(movie, np_items[movie_id])"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "id": "r4qeCtmZtjDx",
        "outputId": "5b4e4aa1-9265-43c5-d154-de628dcae340"
      },
      "source": [
        "sim_pd = pd.DataFrame(similarity_dict.items(), columns=['movieId', 'similarity'])\r\n",
        "sim_pd = sim_pd.merge(movies_pd, on='movieId')\r\n",
        "sim_pd.sort_values('similarity', ascending=False)[['similarity', 'title', 'genres']][:10]"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>similarity</th>\n",
              "      <th>title</th>\n",
              "      <th>genres</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>257</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>Pulp Fiction (1994)</td>\n",
              "      <td>Comedy|Crime|Drama|Thriller</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123</th>\n",
              "      <td>0.355252</td>\n",
              "      <td>Apollo 13 (1995)</td>\n",
              "      <td>Adventure|Drama|IMAX</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2812</th>\n",
              "      <td>0.345988</td>\n",
              "      <td>F/X2 (a.k.a. F/X 2 - The Deadly Art of Illusio...</td>\n",
              "      <td>Action|Crime|Thriller</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9429</th>\n",
              "      <td>0.333559</td>\n",
              "      <td>Microwave Massacre (1983)</td>\n",
              "      <td>Horror</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>211</th>\n",
              "      <td>0.319456</td>\n",
              "      <td>Heavenly Creatures (1994)</td>\n",
              "      <td>Crime|Drama</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>794</th>\n",
              "      <td>0.316977</td>\n",
              "      <td>Lawnmower Man, The (1992)</td>\n",
              "      <td>Action|Horror|Sci-Fi|Thriller</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3907</th>\n",
              "      <td>0.315574</td>\n",
              "      <td>Robin and Marian (1976)</td>\n",
              "      <td>Adventure|Drama|Romance</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>134</th>\n",
              "      <td>0.314788</td>\n",
              "      <td>Crimson Tide (1995)</td>\n",
              "      <td>Drama|Thriller|War</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9334</th>\n",
              "      <td>0.314492</td>\n",
              "      <td>The Infiltrator (2016)</td>\n",
              "      <td>Crime|Drama</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8083</th>\n",
              "      <td>0.313482</td>\n",
              "      <td>ABCs of Death, The (2012)</td>\n",
              "      <td>Horror</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      similarity  ...                         genres\n",
              "257     1.000000  ...    Comedy|Crime|Drama|Thriller\n",
              "123     0.355252  ...           Adventure|Drama|IMAX\n",
              "2812    0.345988  ...          Action|Crime|Thriller\n",
              "9429    0.333559  ...                         Horror\n",
              "211     0.319456  ...                    Crime|Drama\n",
              "794     0.316977  ...  Action|Horror|Sci-Fi|Thriller\n",
              "3907    0.315574  ...        Adventure|Drama|Romance\n",
              "134     0.314788  ...             Drama|Thriller|War\n",
              "9334    0.314492  ...                    Crime|Drama\n",
              "8083    0.313482  ...                         Horror\n",
              "\n",
              "[10 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViFQ4vm90HBi"
      },
      "source": [
        "Выберем произвольного пользователя, получим для него предсказания фильмов, которые он ещё не видел, и выведем топ 30 фильмов по его предсказанной оценке"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7H769l510R7h",
        "outputId": "cff6c757-16de-4cd7-a665-cd3bef357377"
      },
      "source": [
        "user_id = 93\r\n",
        "np_user = np_users[user_id]\r\n",
        "watched_films = np.transpose(data_pd[data_pd['userId'] == user_dict[user_id]][['movieId']].to_numpy())[0]\r\n",
        "len(watched_films)"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "97"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jIOe4nW3O_c",
        "outputId": "0316919d-5818-4c63-959b-1338d9987ae6"
      },
      "source": [
        "np.sort(data_pd[data_pd['userId'] == user_dict[user_id]]['rating'].to_numpy())"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3., 3., 3., 3., 3., 3., 3., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 5., 5., 5., 5., 5., 5.,\n",
              "       5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5.,\n",
              "       5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        },
        "id": "xvFE4yPa1Ilr",
        "outputId": "4bb8fb86-61c5-466f-b237-9c9716a7d165"
      },
      "source": [
        "pred = np_items.dot(np_users[user_id])\r\n",
        "rate_dict = {}\r\n",
        "user_bias = np_user_bias[user_id][0]\r\n",
        "for pred, item_bias, id in zip(pred, np_item_bias, movie_dict.keys()):\r\n",
        "  if not id in watched_films:\r\n",
        "    rate_dict[id] = pred + item_bias[0] + user_bias\r\n",
        "\r\n",
        "ratings_pd = pd.DataFrame(rate_dict.items(), columns=['movieId', 'rating'])\r\n",
        "ratings_pd = ratings_pd.merge(movies_pd, on='movieId')\r\n",
        "\r\n",
        "ratings_pd.sort_values('rating', ascending=False)[['rating', 'title', 'genres']].set_index(np.arange(1, len(ratings_pd)+1))[:30]"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rating</th>\n",
              "      <th>title</th>\n",
              "      <th>genres</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.818322</td>\n",
              "      <td>Shawshank Redemption, The (1994)</td>\n",
              "      <td>Crime|Drama</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.704714</td>\n",
              "      <td>Pulp Fiction (1994)</td>\n",
              "      <td>Comedy|Crime|Drama|Thriller</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2.686809</td>\n",
              "      <td>Forrest Gump (1994)</td>\n",
              "      <td>Comedy|Drama|Romance|War</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.630515</td>\n",
              "      <td>Silence of the Lambs, The (1991)</td>\n",
              "      <td>Crime|Horror|Thriller</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2.612348</td>\n",
              "      <td>Matrix, The (1999)</td>\n",
              "      <td>Action|Sci-Fi|Thriller</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2.574911</td>\n",
              "      <td>Schindler's List (1993)</td>\n",
              "      <td>Drama|War</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2.552903</td>\n",
              "      <td>Braveheart (1995)</td>\n",
              "      <td>Action|Drama|War</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2.535221</td>\n",
              "      <td>Apollo 13 (1995)</td>\n",
              "      <td>Adventure|Drama|IMAX</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2.529496</td>\n",
              "      <td>Jurassic Park (1993)</td>\n",
              "      <td>Action|Adventure|Sci-Fi|Thriller</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>2.504925</td>\n",
              "      <td>Terminator 2: Judgment Day (1991)</td>\n",
              "      <td>Action|Sci-Fi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>2.486291</td>\n",
              "      <td>Fugitive, The (1993)</td>\n",
              "      <td>Thriller</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2.467211</td>\n",
              "      <td>Star Wars: Episode IV - A New Hope (1977)</td>\n",
              "      <td>Action|Adventure|Sci-Fi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2.433513</td>\n",
              "      <td>Dances with Wolves (1990)</td>\n",
              "      <td>Adventure|Drama|Western</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>2.411957</td>\n",
              "      <td>Fight Club (1999)</td>\n",
              "      <td>Action|Crime|Drama|Thriller</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>2.409202</td>\n",
              "      <td>Usual Suspects, The (1995)</td>\n",
              "      <td>Crime|Mystery|Thriller</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>2.404821</td>\n",
              "      <td>Seven (a.k.a. Se7en) (1995)</td>\n",
              "      <td>Mystery|Thriller</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>2.390172</td>\n",
              "      <td>Independence Day (a.k.a. ID4) (1996)</td>\n",
              "      <td>Action|Adventure|Sci-Fi|Thriller</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>2.388660</td>\n",
              "      <td>True Lies (1994)</td>\n",
              "      <td>Action|Adventure|Comedy|Romance|Thriller</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>2.369898</td>\n",
              "      <td>Aladdin (1992)</td>\n",
              "      <td>Adventure|Animation|Children|Comedy|Musical</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>2.334904</td>\n",
              "      <td>Godfather, The (1972)</td>\n",
              "      <td>Crime|Drama</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>2.326146</td>\n",
              "      <td>Die Hard: With a Vengeance (1995)</td>\n",
              "      <td>Action|Crime|Thriller</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>2.318617</td>\n",
              "      <td>Ace Ventura: Pet Detective (1994)</td>\n",
              "      <td>Comedy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>2.305050</td>\n",
              "      <td>Raiders of the Lost Ark (Indiana Jones and the...</td>\n",
              "      <td>Action|Adventure</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>2.302358</td>\n",
              "      <td>Lord of the Rings: The Fellowship of the Ring,...</td>\n",
              "      <td>Adventure|Fantasy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>2.300203</td>\n",
              "      <td>Star Wars: Episode V - The Empire Strikes Back...</td>\n",
              "      <td>Action|Adventure|Sci-Fi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>2.296642</td>\n",
              "      <td>American Beauty (1999)</td>\n",
              "      <td>Drama|Romance</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>2.290971</td>\n",
              "      <td>Saving Private Ryan (1998)</td>\n",
              "      <td>Action|Drama|War</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>2.287262</td>\n",
              "      <td>Lord of the Rings: The Two Towers, The (2002)</td>\n",
              "      <td>Adventure|Fantasy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>2.285477</td>\n",
              "      <td>Lion King, The (1994)</td>\n",
              "      <td>Adventure|Animation|Children|Drama|Musical|IMAX</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>2.266874</td>\n",
              "      <td>Speed (1994)</td>\n",
              "      <td>Action|Romance|Thriller</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      rating  ...                                           genres\n",
              "1   2.818322  ...                                      Crime|Drama\n",
              "2   2.704714  ...                      Comedy|Crime|Drama|Thriller\n",
              "3   2.686809  ...                         Comedy|Drama|Romance|War\n",
              "4   2.630515  ...                            Crime|Horror|Thriller\n",
              "5   2.612348  ...                           Action|Sci-Fi|Thriller\n",
              "6   2.574911  ...                                        Drama|War\n",
              "7   2.552903  ...                                 Action|Drama|War\n",
              "8   2.535221  ...                             Adventure|Drama|IMAX\n",
              "9   2.529496  ...                 Action|Adventure|Sci-Fi|Thriller\n",
              "10  2.504925  ...                                    Action|Sci-Fi\n",
              "11  2.486291  ...                                         Thriller\n",
              "12  2.467211  ...                          Action|Adventure|Sci-Fi\n",
              "13  2.433513  ...                          Adventure|Drama|Western\n",
              "14  2.411957  ...                      Action|Crime|Drama|Thriller\n",
              "15  2.409202  ...                           Crime|Mystery|Thriller\n",
              "16  2.404821  ...                                 Mystery|Thriller\n",
              "17  2.390172  ...                 Action|Adventure|Sci-Fi|Thriller\n",
              "18  2.388660  ...         Action|Adventure|Comedy|Romance|Thriller\n",
              "19  2.369898  ...      Adventure|Animation|Children|Comedy|Musical\n",
              "20  2.334904  ...                                      Crime|Drama\n",
              "21  2.326146  ...                            Action|Crime|Thriller\n",
              "22  2.318617  ...                                           Comedy\n",
              "23  2.305050  ...                                 Action|Adventure\n",
              "24  2.302358  ...                                Adventure|Fantasy\n",
              "25  2.300203  ...                          Action|Adventure|Sci-Fi\n",
              "26  2.296642  ...                                    Drama|Romance\n",
              "27  2.290971  ...                                 Action|Drama|War\n",
              "28  2.287262  ...                                Adventure|Fantasy\n",
              "29  2.285477  ...  Adventure|Animation|Children|Drama|Musical|IMAX\n",
              "30  2.266874  ...                          Action|Romance|Thriller\n",
              "\n",
              "[30 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    }
  ]
}